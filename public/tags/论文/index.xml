<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>论文 on Jagger&#39;s Blog</title>
    <link>https://fireflyyh.top/tags/%E8%AE%BA%E6%96%87/</link>
    <description>Recent content in 论文 on Jagger&#39;s Blog</description>
    <image>
      <title>Jagger&#39;s Blog</title>
      <url>https://fireflyyh.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://fireflyyh.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.129.0</generator>
    <language>en</language>
    <lastBuildDate>Wed, 06 Nov 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://fireflyyh.top/tags/%E8%AE%BA%E6%96%87/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[论文翻译]In Search of an Understandable Consensus Algorithm  (Extended Version)</title>
      <link>https://fireflyyh.top/posts/distributionsystem/raft/</link>
      <pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://fireflyyh.top/posts/distributionsystem/raft/</guid>
      <description>Diego Ongaro and John Ousterhout Stanford University摘要 Raft 是一种用于管理复制日志的共识算法。它能够产生与(多)Paxos 相同的结果，并且效率与 Paxos 相当，但其结构与 Paxos 不同。这种结构上的差异使得 Raft 比 Paxos 更容易理解，同时也为构建实际系统提供了更好的基础。为了提升可理解性，Raft 将共识的关键要素进行了分离，如领导者选举、日志复制和安全性保证等，并且强化了一致性要求以减少需要考虑的状态数量。用户研究的结果表明，相比 Paxos，学生们更容易掌握 Raft。此外，Raft 还引入了一种新的集群成员变更机制，该机制通过使用重叠的多数派来保证安全性。
1 引言 共识算法使得一组机器能够作为一个连贯的整体工作，并且能够在部分成员发生故障时继续运行。正因如此，这类算法在构建可靠的大规模软件系统中发挥着关键作用。在过去的十年里，Paxos1 2主导了关于共识算法的讨论：大多数共识算法的实现要么基于 Paxos，要么受到它的影响，而且 Paxos 已经成为向学生教授共识知识的主要载体。
不幸的是，尽管已经有许多让它变得更容易理解的尝试，Paxos 仍然非常难以理解。此外，它的架构需要进行复杂的改动才能支持实际系统。因此，系统构建者和学生都在与 Paxos 作斗争。
在我们自己也经历过与 Paxos 搏斗之后，我们着手寻找一种新的共识算法，希望它能为系统构建和教育提供更好的基础。我们采用了一种不同寻常的方式，即将 可理解性（understandability） 作为首要目标：我们能否定义一个用于实际系统的共识算法，并以一种比 Paxos 容易得多的方式来描述它？此外，我们希望这个算法能够帮助系统构建者形成必要的直观认识。重要的不仅是算法能够工作，更重要的是能够清楚地理解为什么它能工作。
这项工作的成果就是一个称为 Raft 的共识算法。在设计 Raft 时，我们应用了特定的技术来提高可理解性，包括分解（Raft 将领导者选举、日志复制和安全性分开）和状态空间简化（相比 Paxos，Raft 减少了不确定性的程度以及服务器之间可能产生不一致的方式）。在两所大学进行的涉及 43 名学生的用户研究表明，Raft 比 Paxos 更容易理解:在学习了这两种算法之后，其中 33 名学生对 Raft 的相关问题回答得比 Paxos 的问题要好。
Raft 在许多方面与现有的共识算法相似（最值得注意的是 Oki 和 Liskov 的 Viewstamped Replication 3 4），但它有几个新颖的特点：</description>
    </item>
    <item>
      <title>[论文翻译]MapReduce: Simplified Data Processing on Large Clusters</title>
      <link>https://fireflyyh.top/posts/distributionsystem/mapreduce/</link>
      <pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://fireflyyh.top/posts/distributionsystem/mapreduce/</guid>
      <description>摘要 MapReduce 是一种用于处理和生成大型数据集的编程模型和相关实现。用户指定一个 map 函数来处理键/值对，生成一组中间键/值对，并指定一个 reduce 函数来合并与同一中间键相关的所有中间值。如本文所示，许多现实世界中的任务都可以用这个模型来表达。
以这种函数式风格编写的程序会自动并行化，并在大型商用机器集群上执行。运行时系统会处理分割输入数据、调度程序在一组机器上的执行、处理机器故障以及管理所需的机器间通信等细节。这样，没有任何并行和分布式系统经验的程序员也能轻松利用大型分布式系统的资源。
我们的 MapReduce 实现运行在大型商用机器集群上，并且具有高度可扩展性：典型的 MapReduce 计算在数千台机器上处理数 TB 的数据。程序员发现该系统易于使用：数百个 MapReduce 程序已经实现，并且每天在 Google 集群上执行超过一千个 MapReduce 作业。
1 引言 在过去五年中，本文作者和 Google 的许多其他人已经实现了数百种特殊用途的计算，这些计算处理大量原始数据（例如抓取的文档、Web 请求日志等），以计算各种派生数据（例如倒排索引、Web 文档图形结构的各种表示、每个主机抓取的页面数量摘要、给定一天中最频繁的查询集等）。大多数此类计算在概念上都很简单。但是，输入数据通常很大，并且计算必须分布在数百或数千台机器上才能在合理的时间内完成。如何并行化计算、分发数据和处理故障的问题共同掩盖了原始的简单计算，并使用大量复杂的代码来处理这些问题。
为了应对这种复杂性，我们设计了一种新的抽象，它允许我们表达我们试图执行的简单计算，但隐藏了库中的并行化、容错、数据分布和负载平衡的复杂细节。我们的抽象受到 Lisp 和许多其他函数式语言中存在的 map 和 Reduce 原语的启发。我们意识到，我们的大多数计算都涉及将 map 操作应用于输入中的每个逻辑“记录”，以计算一组中间键/值对，然后对共享相同键的所有值应用 Reduce 操作，以便适当地组合派生数据。我们使用具有用户指定的 map 和 Reduce 操作的函数模型，使我们能够轻松地并行化大型计算，并使用重新执行作为容错的主要机制。
这项工作的主要贡献在于提供了一个简单而功能强大的接口，可实现大规模计算的自动并行化和分布式处理，同时还实现了该接口在大型商用 PC 集群上的高性能。
第 2 节描述了基本编程模型并给出了几个示例。第 3 节描述了针对我们基于集群的计算环境定制的 MapReduce 接口实现。第 4 节描述了我们发现有用的编程模型的几个改进。第 5 节对我们的实现进行了各种任务的性能测量。第 6 节探讨了 MapReduce 在 Google 中的使用情况，包括我们使用它作为重写生产索引系统的基础的经验。第 7 节讨论了相关工作和未来工作。
2 编程模型 计算接收一组输入键/值对，并产生一组输出键/值对。MapReduce 库的用户将计算表述为两个函数：Map 和 Reduce。</description>
    </item>
    <item>
      <title>[论文翻译]The Google File System</title>
      <link>https://fireflyyh.top/posts/distributionsystem/gfs/</link>
      <pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://fireflyyh.top/posts/distributionsystem/gfs/</guid>
      <description>摘要 我们设计并实现了谷歌文件系统，这是一个可扩展的分布式文件系统，适用于大型分布式数据密集型应用。该系统可在廉价的商品硬件上运行，同时提供容错功能，并能为大量客户端提供高聚合性能。
我们的设计目标与之前的分布式文件系统有许多相同之处，但我们对应用工作负载和技术环境（包括当前和预期环境）的观察结果表明，我们的设计明显偏离了之前的一些文件系统假设。这促使我们重新审视传统的选择，探索完全不同的设计要点。
文件系统成功地满足了我们的存储需求。它在谷歌内部被广泛部署，作为生成和处理我们的服务所使用的数据以及需要大型数据集的研发工作的存储平台。迄今为止，最大的集群在一千多台机器上的数千个磁盘上提供了数百 TB 的存储空间，并被数百个客户端并发访问。
在本文中，我们介绍了为支持分布式应用而设计的文件系统接口扩展，讨论了我们设计的许多方面，并报告了微基准测试和实际使用的测量结果。
1. 引言 我们设计并实施了谷歌文件系统（GFS），以满足谷歌快速增长的数据处理需求。GFS 与以前的分布式文件系统有许多相同的目标，如性能、可扩展性、可靠性和可用性。但是，在设计 GFS 时，我们对当前和预期的应用工作负载和技术环境进行了重要观察，这反映出我们明显偏离了之前的一些文件系统设计假设。我们重新审视了传统的选择，并探索了设计空间中完全不同的点。
首先，组件故障是常态而非例外。文件系统由数百甚至数千台存储机组成，这些存储机都是用廉价的商品部件制造的，并被数量相当的客户机访问。这些组件的数量和质量几乎可以保证，在任何特定时间都会有一些组件无法正常工作，而且有些组件无法从当前故障中恢复。我们见过应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源故障造成的问题。因此，持续监控、错误检测、容错和自动恢复必须成为系统的组成部分。
其次，按照传统的标准，文件是巨大的。多 GB 的文件很常见。每个文件通常包含许多应用对象，如网络文档。当我们经常处理由数十亿个对象组成的多 TB 快速增长的数据集时，即使文件系统可以支持，要管理数十亿个约 KB 大小的文件也很不方便。因此，必须重新审视 I/O 操作和块大小等设计假设和参数。
第三，大多数文件都是通过添加新数据而不是覆盖现有数据来改变的。文件内的随机写入几乎不存在。文件一旦写入，就只能读取，而且通常只能按顺序读取。各种数据都具有这些特征。有些可能是数据分析程序扫描过的大型存储库。有些可能是运行中的应用程序持续生成的数据流。有些可能是档案数据。有些可能是在一台机器上生成并在另一台机器上处理的中间结果，无论是同时处理还是稍后处理。鉴于巨型文件的这种访问模式，追加成为性能优化和原子性保证的重点，而在客户端缓存数据块则失去了吸引力。
第四，共同设计应用程序和文件系统 API 可以提高我们的灵活性，从而使整个系统受益。例如，我们放宽了 GFS 的一致性模型，大大简化了文件系统，而不会给应用程序带来沉重负担。我们还引入了原子追加操作，使多个客户端可以同时追加文件，而无需额外的同步。本文稍后将详细讨论这些内容。
目前，为不同目的部署了多个 GFS 集群。最大的集群有超过 1000 个存储节点，超过 300 TB 的磁盘存储空间，数百个客户端在不同的机器上持续大量访问。
2. 设计概述 2.1 假设 在设计满足我们需求的文件系统时，我们遵循的假设既是挑战也是机遇。我们在前面提到了一些重要的观察结果，现在详细介绍一下我们的假设。
该系统由许多经常发生故障的廉价商品组件构成。系统必须不断进行自我监控，日常检测、容忍和及时恢复组件故障。 系统存储的大文件数量不多。我们预计会有几百万个文件，每个文件的大小通常为 100 MB 或更大。多 GB 文件是常见情况，应得到有效管理。我们必须支持小文件，但无需对其进行优化。 工作负载主要包括两种读取：大型流式读取和小型随机读取。在大数据流读取中，单个操作通常读取数百 KB，更常见的是 1 MB 或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小型随机读取通常在某个任意偏移位置读取几个 KB 的数据。注重性能的应用程序通常会对小规模读取进行批处理和排序，以稳定地读取文件，而不是来回读取。 这些工作负载中还有许多向文件追加数据的大型连续写入操作。典型的操作大小与读取类似。文件一旦写入，就很少再修改。支持在文件任意位置进行小规模写入，但不一定要高效。 系统必须有效地实现多个客户端同时追加到同一文件的定义明确的语义。我们的文件通常用作生产者-消费者队列或多路合并。数以百计的生产者（每台机器运行一个）将同时追加到一个文件。同步开销最小的原子性至关重要。文件可能会在稍后被读取，或者消费者可能会同时读取文件。 高持续带宽比低延迟更重要。我们的大多数目标应用都非常重视高速批量处理数据，而很少有应用对单个读取或写入的响应时间有严格要求。 2.2 接口 GFS 提供了一个熟悉的文件系统接口，尽管它没有实现标准的 API（如 POSIX）。文件在目录中按层次组织，并用路径名标识。我们支持创建、删除、打开、关闭、读取和写入文件的常规操作。
此外，GFS 还有快照和记录追加操作。快照以低成本创建文件或目录树的副本。记录追加允许多个客户端同时向同一个文件追加数据，同时保证每个客户端追加的原子性。它适用于实现多向合并结果和生产者-消费者队列，许多客户端可以同时追加数据而无需额外锁定。我们发现，这些类型的文件在构建大型分布式应用时非常有用。快照追加和记录追加将分别在第 3.4 节和第 3.3 节中进一步讨论。</description>
    </item>
  </channel>
</rss>
