<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The Google File System | Jagger's Blog</title>
<meta name=keywords content="分布式,论文"><meta name=description content="摘要 我们设计并实现了谷歌文件系统，这是一个可扩展的分布式文件系统，适用于大型分布式数据密集型应用。该系统可在廉价的商品硬件上运行，同时提供容错功能，并能为大量客户端提供高聚合性能。
我们的设计目标与之前的分布式文件系统有许多相同之处，但我们对应用工作负载和技术环境（包括当前和预期环境）的观察结果表明，我们的设计明显偏离了之前的一些文件系统假设。这促使我们重新审视传统的选择，探索完全不同的设计要点。
文件系统成功地满足了我们的存储需求。它在谷歌内部被广泛部署，作为生成和处理我们的服务所使用的数据以及需要大型数据集的研发工作的存储平台。迄今为止，最大的集群在一千多台机器上的数千个磁盘上提供了数百 TB 的存储空间，并被数百个客户端并发访问。
在本文中，我们介绍了为支持分布式应用而设计的文件系统接口扩展，讨论了我们设计的许多方面，并报告了微基准测试和实际使用的测量结果。
1. 引言 我们设计并实施了谷歌文件系统（GFS），以满足谷歌快速增长的数据处理需求。GFS 与以前的分布式文件系统有许多相同的目标，如性能、可扩展性、可靠性和可用性。但是，在设计 GFS 时，我们对当前和预期的应用工作负载和技术环境进行了重要观察，这反映出我们明显偏离了之前的一些文件系统设计假设。我们重新审视了传统的选择，并探索了设计空间中完全不同的点。
首先，组件故障是常态而非例外。文件系统由数百甚至数千台存储机组成，这些存储机都是用廉价的商品部件制造的，并被数量相当的客户机访问。这些组件的数量和质量几乎可以保证，在任何特定时间都会有一些组件无法正常工作，而且有些组件无法从当前故障中恢复。我们见过应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源故障造成的问题。因此，持续监控、错误检测、容错和自动恢复必须成为系统的组成部分。
其次，按照传统的标准，文件是巨大的。多 GB 的文件很常见。每个文件通常包含许多应用对象，如网络文档。当我们经常处理由数十亿个对象组成的多 TB 快速增长的数据集时，即使文件系统可以支持，要管理数十亿个约 KB 大小的文件也很不方便。因此，必须重新审视 I/O 操作和块大小等设计假设和参数。
第三，大多数文件都是通过添加新数据而不是覆盖现有数据来改变的。文件内的随机写入几乎不存在。文件一旦写入，就只能读取，而且通常只能按顺序读取。各种数据都具有这些特征。有些可能是数据分析程序扫描过的大型存储库。有些可能是运行中的应用程序持续生成的数据流。有些可能是档案数据。有些可能是在一台机器上生成并在另一台机器上处理的中间结果，无论是同时处理还是稍后处理。鉴于巨型文件的这种访问模式，追加成为性能优化和原子性保证的重点，而在客户端缓存数据块则失去了吸引力。
第四，共同设计应用程序和文件系统 API 可以提高我们的灵活性，从而使整个系统受益。例如，我们放宽了 GFS 的一致性模型，大大简化了文件系统，而不会给应用程序带来沉重负担。我们还引入了原子追加操作，使多个客户端可以同时追加文件，而无需额外的同步。本文稍后将详细讨论这些内容。
目前，为不同目的部署了多个 GFS 集群。最大的集群有超过 1000 个存储节点，超过 300 TB 的磁盘存储空间，数百个客户端在不同的机器上持续大量访问。
2. 设计概述 2.1 假设 在设计满足我们需求的文件系统时，我们遵循的假设既是挑战也是机遇。我们在前面提到了一些重要的观察结果，现在详细介绍一下我们的假设。
该系统由许多经常发生故障的廉价商品组件构成。系统必须不断进行自我监控，日常检测、容忍和及时恢复组件故障。 系统存储的大文件数量不多。我们预计会有几百万个文件，每个文件的大小通常为 100 MB 或更大。多 GB 文件是常见情况，应得到有效管理。我们必须支持小文件，但无需对其进行优化。 工作负载主要包括两种读取：大型流式读取和小型随机读取。在大数据流读取中，单个操作通常读取数百 KB，更常见的是 1 MB 或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小型随机读取通常在某个任意偏移位置读取几个 KB 的数据。注重性能的应用程序通常会对小规模读取进行批处理和排序，以稳定地读取文件，而不是来回读取。 这些工作负载中还有许多向文件追加数据的大型连续写入操作。典型的操作大小与读取类似。文件一旦写入，就很少再修改。支持在文件任意位置进行小规模写入，但不一定要高效。 系统必须有效地实现多个客户端同时追加到同一文件的定义明确的语义。我们的文件通常用作生产者-消费者队列或多路合并。数以百计的生产者（每台机器运行一个）将同时追加到一个文件。同步开销最小的原子性至关重要。文件可能会在稍后被读取，或者消费者可能会同时读取文件。 高持续带宽比低延迟更重要。我们的大多数目标应用都非常重视高速批量处理数据，而很少有应用对单个读取或写入的响应时间有严格要求。 2.2 接口 GFS 提供了一个熟悉的文件系统接口，尽管它没有实现标准的 API（如 POSIX）。文件在目录中按层次组织，并用路径名标识。我们支持创建、删除、打开、关闭、读取和写入文件的常规操作。
此外，GFS 还有快照和记录追加操作。快照以低成本创建文件或目录树的副本。记录追加允许多个客户端同时向同一个文件追加数据，同时保证每个客户端追加的原子性。它适用于实现多向合并结果和生产者-消费者队列，许多客户端可以同时追加数据而无需额外锁定。我们发现，这些类型的文件在构建大型分布式应用时非常有用。快照追加和记录追加将分别在第 3.4 节和第 3.3 节中进一步讨论。"><meta name=author content="Jagger"><link rel=canonical href=https://fireflyyh.top/posts/distributionsystem/gfs%E7%BF%BB%E8%AF%91/><link crossorigin=anonymous href=/assets/css/stylesheet.1212f0454c490726d67933de27a90315a6177ba895fb506fe0f2bd6e93b66acd.css integrity="sha256-EhLwRUxJBybWeTPeJ6kDFaYXe6iV+1Bv4PK9bpO2as0=" rel="preload stylesheet" as=style><link rel=icon href=https://fireflyyh.top/img/yuan.jpg><link rel=icon type=image/png sizes=16x16 href=https://fireflyyh.top/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://fireflyyh.top/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://fireflyyh.top/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://fireflyyh.top/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://fireflyyh.top/posts/distributionsystem/gfs%E7%BF%BB%E8%AF%91/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?87f1888ba5320e03987ecde7525fde2d",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><script type=text/javascript async src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y30PKHXBN4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y30PKHXBN4")</script><meta property="og:title" content="The Google File System"><meta property="og:description" content="摘要 我们设计并实现了谷歌文件系统，这是一个可扩展的分布式文件系统，适用于大型分布式数据密集型应用。该系统可在廉价的商品硬件上运行，同时提供容错功能，并能为大量客户端提供高聚合性能。
我们的设计目标与之前的分布式文件系统有许多相同之处，但我们对应用工作负载和技术环境（包括当前和预期环境）的观察结果表明，我们的设计明显偏离了之前的一些文件系统假设。这促使我们重新审视传统的选择，探索完全不同的设计要点。
文件系统成功地满足了我们的存储需求。它在谷歌内部被广泛部署，作为生成和处理我们的服务所使用的数据以及需要大型数据集的研发工作的存储平台。迄今为止，最大的集群在一千多台机器上的数千个磁盘上提供了数百 TB 的存储空间，并被数百个客户端并发访问。
在本文中，我们介绍了为支持分布式应用而设计的文件系统接口扩展，讨论了我们设计的许多方面，并报告了微基准测试和实际使用的测量结果。
1. 引言 我们设计并实施了谷歌文件系统（GFS），以满足谷歌快速增长的数据处理需求。GFS 与以前的分布式文件系统有许多相同的目标，如性能、可扩展性、可靠性和可用性。但是，在设计 GFS 时，我们对当前和预期的应用工作负载和技术环境进行了重要观察，这反映出我们明显偏离了之前的一些文件系统设计假设。我们重新审视了传统的选择，并探索了设计空间中完全不同的点。
首先，组件故障是常态而非例外。文件系统由数百甚至数千台存储机组成，这些存储机都是用廉价的商品部件制造的，并被数量相当的客户机访问。这些组件的数量和质量几乎可以保证，在任何特定时间都会有一些组件无法正常工作，而且有些组件无法从当前故障中恢复。我们见过应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源故障造成的问题。因此，持续监控、错误检测、容错和自动恢复必须成为系统的组成部分。
其次，按照传统的标准，文件是巨大的。多 GB 的文件很常见。每个文件通常包含许多应用对象，如网络文档。当我们经常处理由数十亿个对象组成的多 TB 快速增长的数据集时，即使文件系统可以支持，要管理数十亿个约 KB 大小的文件也很不方便。因此，必须重新审视 I/O 操作和块大小等设计假设和参数。
第三，大多数文件都是通过添加新数据而不是覆盖现有数据来改变的。文件内的随机写入几乎不存在。文件一旦写入，就只能读取，而且通常只能按顺序读取。各种数据都具有这些特征。有些可能是数据分析程序扫描过的大型存储库。有些可能是运行中的应用程序持续生成的数据流。有些可能是档案数据。有些可能是在一台机器上生成并在另一台机器上处理的中间结果，无论是同时处理还是稍后处理。鉴于巨型文件的这种访问模式，追加成为性能优化和原子性保证的重点，而在客户端缓存数据块则失去了吸引力。
第四，共同设计应用程序和文件系统 API 可以提高我们的灵活性，从而使整个系统受益。例如，我们放宽了 GFS 的一致性模型，大大简化了文件系统，而不会给应用程序带来沉重负担。我们还引入了原子追加操作，使多个客户端可以同时追加文件，而无需额外的同步。本文稍后将详细讨论这些内容。
目前，为不同目的部署了多个 GFS 集群。最大的集群有超过 1000 个存储节点，超过 300 TB 的磁盘存储空间，数百个客户端在不同的机器上持续大量访问。
2. 设计概述 2.1 假设 在设计满足我们需求的文件系统时，我们遵循的假设既是挑战也是机遇。我们在前面提到了一些重要的观察结果，现在详细介绍一下我们的假设。
该系统由许多经常发生故障的廉价商品组件构成。系统必须不断进行自我监控，日常检测、容忍和及时恢复组件故障。 系统存储的大文件数量不多。我们预计会有几百万个文件，每个文件的大小通常为 100 MB 或更大。多 GB 文件是常见情况，应得到有效管理。我们必须支持小文件，但无需对其进行优化。 工作负载主要包括两种读取：大型流式读取和小型随机读取。在大数据流读取中，单个操作通常读取数百 KB，更常见的是 1 MB 或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小型随机读取通常在某个任意偏移位置读取几个 KB 的数据。注重性能的应用程序通常会对小规模读取进行批处理和排序，以稳定地读取文件，而不是来回读取。 这些工作负载中还有许多向文件追加数据的大型连续写入操作。典型的操作大小与读取类似。文件一旦写入，就很少再修改。支持在文件任意位置进行小规模写入，但不一定要高效。 系统必须有效地实现多个客户端同时追加到同一文件的定义明确的语义。我们的文件通常用作生产者-消费者队列或多路合并。数以百计的生产者（每台机器运行一个）将同时追加到一个文件。同步开销最小的原子性至关重要。文件可能会在稍后被读取，或者消费者可能会同时读取文件。 高持续带宽比低延迟更重要。我们的大多数目标应用都非常重视高速批量处理数据，而很少有应用对单个读取或写入的响应时间有严格要求。 2.2 接口 GFS 提供了一个熟悉的文件系统接口，尽管它没有实现标准的 API（如 POSIX）。文件在目录中按层次组织，并用路径名标识。我们支持创建、删除、打开、关闭、读取和写入文件的常规操作。
此外，GFS 还有快照和记录追加操作。快照以低成本创建文件或目录树的副本。记录追加允许多个客户端同时向同一个文件追加数据，同时保证每个客户端追加的原子性。它适用于实现多向合并结果和生产者-消费者队列，许多客户端可以同时追加数据而无需额外锁定。我们发现，这些类型的文件在构建大型分布式应用时非常有用。快照追加和记录追加将分别在第 3.4 节和第 3.3 节中进一步讨论。"><meta property="og:type" content="article"><meta property="og:url" content="https://fireflyyh.top/posts/distributionsystem/gfs%E7%BF%BB%E8%AF%91/"><meta property="og:image" content="https://fireflyyh.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-24T00:00:00+00:00"><meta property="article:modified_time" content="2024-10-24T00:00:00+00:00"><meta property="og:site_name" content="Jagger's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fireflyyh.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="The Google File System"><meta name=twitter:description content="摘要 我们设计并实现了谷歌文件系统，这是一个可扩展的分布式文件系统，适用于大型分布式数据密集型应用。该系统可在廉价的商品硬件上运行，同时提供容错功能，并能为大量客户端提供高聚合性能。
我们的设计目标与之前的分布式文件系统有许多相同之处，但我们对应用工作负载和技术环境（包括当前和预期环境）的观察结果表明，我们的设计明显偏离了之前的一些文件系统假设。这促使我们重新审视传统的选择，探索完全不同的设计要点。
文件系统成功地满足了我们的存储需求。它在谷歌内部被广泛部署，作为生成和处理我们的服务所使用的数据以及需要大型数据集的研发工作的存储平台。迄今为止，最大的集群在一千多台机器上的数千个磁盘上提供了数百 TB 的存储空间，并被数百个客户端并发访问。
在本文中，我们介绍了为支持分布式应用而设计的文件系统接口扩展，讨论了我们设计的许多方面，并报告了微基准测试和实际使用的测量结果。
1. 引言 我们设计并实施了谷歌文件系统（GFS），以满足谷歌快速增长的数据处理需求。GFS 与以前的分布式文件系统有许多相同的目标，如性能、可扩展性、可靠性和可用性。但是，在设计 GFS 时，我们对当前和预期的应用工作负载和技术环境进行了重要观察，这反映出我们明显偏离了之前的一些文件系统设计假设。我们重新审视了传统的选择，并探索了设计空间中完全不同的点。
首先，组件故障是常态而非例外。文件系统由数百甚至数千台存储机组成，这些存储机都是用廉价的商品部件制造的，并被数量相当的客户机访问。这些组件的数量和质量几乎可以保证，在任何特定时间都会有一些组件无法正常工作，而且有些组件无法从当前故障中恢复。我们见过应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源故障造成的问题。因此，持续监控、错误检测、容错和自动恢复必须成为系统的组成部分。
其次，按照传统的标准，文件是巨大的。多 GB 的文件很常见。每个文件通常包含许多应用对象，如网络文档。当我们经常处理由数十亿个对象组成的多 TB 快速增长的数据集时，即使文件系统可以支持，要管理数十亿个约 KB 大小的文件也很不方便。因此，必须重新审视 I/O 操作和块大小等设计假设和参数。
第三，大多数文件都是通过添加新数据而不是覆盖现有数据来改变的。文件内的随机写入几乎不存在。文件一旦写入，就只能读取，而且通常只能按顺序读取。各种数据都具有这些特征。有些可能是数据分析程序扫描过的大型存储库。有些可能是运行中的应用程序持续生成的数据流。有些可能是档案数据。有些可能是在一台机器上生成并在另一台机器上处理的中间结果，无论是同时处理还是稍后处理。鉴于巨型文件的这种访问模式，追加成为性能优化和原子性保证的重点，而在客户端缓存数据块则失去了吸引力。
第四，共同设计应用程序和文件系统 API 可以提高我们的灵活性，从而使整个系统受益。例如，我们放宽了 GFS 的一致性模型，大大简化了文件系统，而不会给应用程序带来沉重负担。我们还引入了原子追加操作，使多个客户端可以同时追加文件，而无需额外的同步。本文稍后将详细讨论这些内容。
目前，为不同目的部署了多个 GFS 集群。最大的集群有超过 1000 个存储节点，超过 300 TB 的磁盘存储空间，数百个客户端在不同的机器上持续大量访问。
2. 设计概述 2.1 假设 在设计满足我们需求的文件系统时，我们遵循的假设既是挑战也是机遇。我们在前面提到了一些重要的观察结果，现在详细介绍一下我们的假设。
该系统由许多经常发生故障的廉价商品组件构成。系统必须不断进行自我监控，日常检测、容忍和及时恢复组件故障。 系统存储的大文件数量不多。我们预计会有几百万个文件，每个文件的大小通常为 100 MB 或更大。多 GB 文件是常见情况，应得到有效管理。我们必须支持小文件，但无需对其进行优化。 工作负载主要包括两种读取：大型流式读取和小型随机读取。在大数据流读取中，单个操作通常读取数百 KB，更常见的是 1 MB 或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小型随机读取通常在某个任意偏移位置读取几个 KB 的数据。注重性能的应用程序通常会对小规模读取进行批处理和排序，以稳定地读取文件，而不是来回读取。 这些工作负载中还有许多向文件追加数据的大型连续写入操作。典型的操作大小与读取类似。文件一旦写入，就很少再修改。支持在文件任意位置进行小规模写入，但不一定要高效。 系统必须有效地实现多个客户端同时追加到同一文件的定义明确的语义。我们的文件通常用作生产者-消费者队列或多路合并。数以百计的生产者（每台机器运行一个）将同时追加到一个文件。同步开销最小的原子性至关重要。文件可能会在稍后被读取，或者消费者可能会同时读取文件。 高持续带宽比低延迟更重要。我们的大多数目标应用都非常重视高速批量处理数据，而很少有应用对单个读取或写入的响应时间有严格要求。 2.2 接口 GFS 提供了一个熟悉的文件系统接口，尽管它没有实现标准的 API（如 POSIX）。文件在目录中按层次组织，并用路径名标识。我们支持创建、删除、打开、关闭、读取和写入文件的常规操作。
此外，GFS 还有快照和记录追加操作。快照以低成本创建文件或目录树的副本。记录追加允许多个客户端同时向同一个文件追加数据，同时保证每个客户端追加的原子性。它适用于实现多向合并结果和生产者-消费者队列，许多客户端可以同时追加数据而无需额外锁定。我们发现，这些类型的文件在构建大型分布式应用时非常有用。快照追加和记录追加将分别在第 3.4 节和第 3.3 节中进一步讨论。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://fireflyyh.top/posts/"},{"@type":"ListItem","position":2,"name":"The Google File System","item":"https://fireflyyh.top/posts/distributionsystem/gfs%E7%BF%BB%E8%AF%91/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Google File System","name":"The Google File System","description":"摘要 我们设计并实现了谷歌文件系统，这是一个可扩展的分布式文件系统，适用于大型分布式数据密集型应用。该系统可在廉价的商品硬件上运行，同时提供容错功能，并能为大量客户端提供高聚合性能。\n我们的设计目标与之前的分布式文件系统有许多相同之处，但我们对应用工作负载和技术环境（包括当前和预期环境）的观察结果表明，我们的设计明显偏离了之前的一些文件系统假设。这促使我们重新审视传统的选择，探索完全不同的设计要点。\n文件系统成功地满足了我们的存储需求。它在谷歌内部被广泛部署，作为生成和处理我们的服务所使用的数据以及需要大型数据集的研发工作的存储平台。迄今为止，最大的集群在一千多台机器上的数千个磁盘上提供了数百 TB 的存储空间，并被数百个客户端并发访问。\n在本文中，我们介绍了为支持分布式应用而设计的文件系统接口扩展，讨论了我们设计的许多方面，并报告了微基准测试和实际使用的测量结果。\n1. 引言 我们设计并实施了谷歌文件系统（GFS），以满足谷歌快速增长的数据处理需求。GFS 与以前的分布式文件系统有许多相同的目标，如性能、可扩展性、可靠性和可用性。但是，在设计 GFS 时，我们对当前和预期的应用工作负载和技术环境进行了重要观察，这反映出我们明显偏离了之前的一些文件系统设计假设。我们重新审视了传统的选择，并探索了设计空间中完全不同的点。\n首先，组件故障是常态而非例外。文件系统由数百甚至数千台存储机组成，这些存储机都是用廉价的商品部件制造的，并被数量相当的客户机访问。这些组件的数量和质量几乎可以保证，在任何特定时间都会有一些组件无法正常工作，而且有些组件无法从当前故障中恢复。我们见过应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源故障造成的问题。因此，持续监控、错误检测、容错和自动恢复必须成为系统的组成部分。\n其次，按照传统的标准，文件是巨大的。多 GB 的文件很常见。每个文件通常包含许多应用对象，如网络文档。当我们经常处理由数十亿个对象组成的多 TB 快速增长的数据集时，即使文件系统可以支持，要管理数十亿个约 KB 大小的文件也很不方便。因此，必须重新审视 I/O 操作和块大小等设计假设和参数。\n第三，大多数文件都是通过添加新数据而不是覆盖现有数据来改变的。文件内的随机写入几乎不存在。文件一旦写入，就只能读取，而且通常只能按顺序读取。各种数据都具有这些特征。有些可能是数据分析程序扫描过的大型存储库。有些可能是运行中的应用程序持续生成的数据流。有些可能是档案数据。有些可能是在一台机器上生成并在另一台机器上处理的中间结果，无论是同时处理还是稍后处理。鉴于巨型文件的这种访问模式，追加成为性能优化和原子性保证的重点，而在客户端缓存数据块则失去了吸引力。\n第四，共同设计应用程序和文件系统 API 可以提高我们的灵活性，从而使整个系统受益。例如，我们放宽了 GFS 的一致性模型，大大简化了文件系统，而不会给应用程序带来沉重负担。我们还引入了原子追加操作，使多个客户端可以同时追加文件，而无需额外的同步。本文稍后将详细讨论这些内容。\n目前，为不同目的部署了多个 GFS 集群。最大的集群有超过 1000 个存储节点，超过 300 TB 的磁盘存储空间，数百个客户端在不同的机器上持续大量访问。\n2. 设计概述 2.1 假设 在设计满足我们需求的文件系统时，我们遵循的假设既是挑战也是机遇。我们在前面提到了一些重要的观察结果，现在详细介绍一下我们的假设。\n该系统由许多经常发生故障的廉价商品组件构成。系统必须不断进行自我监控，日常检测、容忍和及时恢复组件故障。 系统存储的大文件数量不多。我们预计会有几百万个文件，每个文件的大小通常为 100 MB 或更大。多 GB 文件是常见情况，应得到有效管理。我们必须支持小文件，但无需对其进行优化。 工作负载主要包括两种读取：大型流式读取和小型随机读取。在大数据流读取中，单个操作通常读取数百 KB，更常见的是 1 MB 或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小型随机读取通常在某个任意偏移位置读取几个 KB 的数据。注重性能的应用程序通常会对小规模读取进行批处理和排序，以稳定地读取文件，而不是来回读取。 这些工作负载中还有许多向文件追加数据的大型连续写入操作。典型的操作大小与读取类似。文件一旦写入，就很少再修改。支持在文件任意位置进行小规模写入，但不一定要高效。 系统必须有效地实现多个客户端同时追加到同一文件的定义明确的语义。我们的文件通常用作生产者-消费者队列或多路合并。数以百计的生产者（每台机器运行一个）将同时追加到一个文件。同步开销最小的原子性至关重要。文件可能会在稍后被读取，或者消费者可能会同时读取文件。 高持续带宽比低延迟更重要。我们的大多数目标应用都非常重视高速批量处理数据，而很少有应用对单个读取或写入的响应时间有严格要求。 2.2 接口 GFS 提供了一个熟悉的文件系统接口，尽管它没有实现标准的 API（如 POSIX）。文件在目录中按层次组织，并用路径名标识。我们支持创建、删除、打开、关闭、读取和写入文件的常规操作。\n此外，GFS 还有快照和记录追加操作。快照以低成本创建文件或目录树的副本。记录追加允许多个客户端同时向同一个文件追加数据，同时保证每个客户端追加的原子性。它适用于实现多向合并结果和生产者-消费者队列，许多客户端可以同时追加数据而无需额外锁定。我们发现，这些类型的文件在构建大型分布式应用时非常有用。快照追加和记录追加将分别在第 3.4 节和第 3.3 节中进一步讨论。","keywords":["分布式","论文"],"articleBody":"摘要 我们设计并实现了谷歌文件系统，这是一个可扩展的分布式文件系统，适用于大型分布式数据密集型应用。该系统可在廉价的商品硬件上运行，同时提供容错功能，并能为大量客户端提供高聚合性能。\n我们的设计目标与之前的分布式文件系统有许多相同之处，但我们对应用工作负载和技术环境（包括当前和预期环境）的观察结果表明，我们的设计明显偏离了之前的一些文件系统假设。这促使我们重新审视传统的选择，探索完全不同的设计要点。\n文件系统成功地满足了我们的存储需求。它在谷歌内部被广泛部署，作为生成和处理我们的服务所使用的数据以及需要大型数据集的研发工作的存储平台。迄今为止，最大的集群在一千多台机器上的数千个磁盘上提供了数百 TB 的存储空间，并被数百个客户端并发访问。\n在本文中，我们介绍了为支持分布式应用而设计的文件系统接口扩展，讨论了我们设计的许多方面，并报告了微基准测试和实际使用的测量结果。\n1. 引言 我们设计并实施了谷歌文件系统（GFS），以满足谷歌快速增长的数据处理需求。GFS 与以前的分布式文件系统有许多相同的目标，如性能、可扩展性、可靠性和可用性。但是，在设计 GFS 时，我们对当前和预期的应用工作负载和技术环境进行了重要观察，这反映出我们明显偏离了之前的一些文件系统设计假设。我们重新审视了传统的选择，并探索了设计空间中完全不同的点。\n首先，组件故障是常态而非例外。文件系统由数百甚至数千台存储机组成，这些存储机都是用廉价的商品部件制造的，并被数量相当的客户机访问。这些组件的数量和质量几乎可以保证，在任何特定时间都会有一些组件无法正常工作，而且有些组件无法从当前故障中恢复。我们见过应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源故障造成的问题。因此，持续监控、错误检测、容错和自动恢复必须成为系统的组成部分。\n其次，按照传统的标准，文件是巨大的。多 GB 的文件很常见。每个文件通常包含许多应用对象，如网络文档。当我们经常处理由数十亿个对象组成的多 TB 快速增长的数据集时，即使文件系统可以支持，要管理数十亿个约 KB 大小的文件也很不方便。因此，必须重新审视 I/O 操作和块大小等设计假设和参数。\n第三，大多数文件都是通过添加新数据而不是覆盖现有数据来改变的。文件内的随机写入几乎不存在。文件一旦写入，就只能读取，而且通常只能按顺序读取。各种数据都具有这些特征。有些可能是数据分析程序扫描过的大型存储库。有些可能是运行中的应用程序持续生成的数据流。有些可能是档案数据。有些可能是在一台机器上生成并在另一台机器上处理的中间结果，无论是同时处理还是稍后处理。鉴于巨型文件的这种访问模式，追加成为性能优化和原子性保证的重点，而在客户端缓存数据块则失去了吸引力。\n第四，共同设计应用程序和文件系统 API 可以提高我们的灵活性，从而使整个系统受益。例如，我们放宽了 GFS 的一致性模型，大大简化了文件系统，而不会给应用程序带来沉重负担。我们还引入了原子追加操作，使多个客户端可以同时追加文件，而无需额外的同步。本文稍后将详细讨论这些内容。\n目前，为不同目的部署了多个 GFS 集群。最大的集群有超过 1000 个存储节点，超过 300 TB 的磁盘存储空间，数百个客户端在不同的机器上持续大量访问。\n2. 设计概述 2.1 假设 在设计满足我们需求的文件系统时，我们遵循的假设既是挑战也是机遇。我们在前面提到了一些重要的观察结果，现在详细介绍一下我们的假设。\n该系统由许多经常发生故障的廉价商品组件构成。系统必须不断进行自我监控，日常检测、容忍和及时恢复组件故障。 系统存储的大文件数量不多。我们预计会有几百万个文件，每个文件的大小通常为 100 MB 或更大。多 GB 文件是常见情况，应得到有效管理。我们必须支持小文件，但无需对其进行优化。 工作负载主要包括两种读取：大型流式读取和小型随机读取。在大数据流读取中，单个操作通常读取数百 KB，更常见的是 1 MB 或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小型随机读取通常在某个任意偏移位置读取几个 KB 的数据。注重性能的应用程序通常会对小规模读取进行批处理和排序，以稳定地读取文件，而不是来回读取。 这些工作负载中还有许多向文件追加数据的大型连续写入操作。典型的操作大小与读取类似。文件一旦写入，就很少再修改。支持在文件任意位置进行小规模写入，但不一定要高效。 系统必须有效地实现多个客户端同时追加到同一文件的定义明确的语义。我们的文件通常用作生产者-消费者队列或多路合并。数以百计的生产者（每台机器运行一个）将同时追加到一个文件。同步开销最小的原子性至关重要。文件可能会在稍后被读取，或者消费者可能会同时读取文件。 高持续带宽比低延迟更重要。我们的大多数目标应用都非常重视高速批量处理数据，而很少有应用对单个读取或写入的响应时间有严格要求。 2.2 接口 GFS 提供了一个熟悉的文件系统接口，尽管它没有实现标准的 API（如 POSIX）。文件在目录中按层次组织，并用路径名标识。我们支持创建、删除、打开、关闭、读取和写入文件的常规操作。\n此外，GFS 还有快照和记录追加操作。快照以低成本创建文件或目录树的副本。记录追加允许多个客户端同时向同一个文件追加数据，同时保证每个客户端追加的原子性。它适用于实现多向合并结果和生产者-消费者队列，许多客户端可以同时追加数据而无需额外锁定。我们发现，这些类型的文件在构建大型分布式应用时非常有用。快照追加和记录追加将分别在第 3.4 节和第 3.3 节中进一步讨论。\n2.3 架构 如图 1 所示，GFS 集群由一个主服务器和多个分块服务器组成，并由多个客户端访问。每个客户端通常是一台运行用户级服务器进程的商用 Linux 机器。只要机器资源允许，并且可以接受运行可能不稳定的应用程序代码所带来的较低可靠性，在同一台机器上同时运行分块服务器和客户端是很容易的。\n文件被分成固定大小的块。每个分块都由主服务器程序在创建分块时分配的不可更改且全球唯一的 64 位分块句柄来标识。分块服务器将分块作为 Linux 文件存储在本地磁盘上，并读取或写入由分块句柄和字节范围指定的分块数据。为了保证可靠性，每个数据块都会在多个数据块服务器上复制。默认情况下，我们会存储三个副本，但用户可以为文件命名空间的不同区域指定不同的复制级别。\n主文件系统维护所有文件系统元数据。其中包括命名空间、访问控制信息、文件到分块的映射以及分块的当前位置。主服务器还控制着全系统的活动，如块租约管理、孤儿块的垃圾回收以及块服务器之间的块迁移。主服务器会定期通过 HeartBeat 消息与每个块服务器进行通信，向其下达指令并收集其状态。\n连接到每个应用程序的 GFS 客户端代码实现文件系统 API，并与主服务器程序和块服务器通信，代表应用程序读取或写入数据。客户端与主服务器交互元数据操作，但所有数据通信都直接与块服务器进行。我们不提供 POSIX API，因此无需连接 Linux vnode 层。\n客户端和分块服务器都不缓存文件数据。客户端缓存的好处不大，因为大多数应用程序都会流式处理大量文件，或者工作集过大而无法缓存。没有缓存可以消除缓存一致性问题，从而简化客户端和整个系统。(但客户端会缓存元数据）。块服务器无需缓存文件数据，因为块是以本地文件的形式存储的，因此 Linux 的缓冲缓存已经将经常访问的数据保存在内存中。\n2.4 单个主服务器 单个主站大大简化了我们的设计，并使主站能够利用全局知识做出复杂的分块放置和复制决策。不过，我们必须尽量减少主服务器程序对读写的参与，以免其成为瓶颈。客户端从不通过主服务器读写文件数据。相反，客户端会询问主服务器它应该联系哪些分块服务器。客户端会在有限的时间内缓存这些信息，并在随后的许多操作中直接与分块服务器交互。\n让我们参照图 1 来解释一下简单读取的交互过程。首先，客户端使用固定的块大小，将应用程序指定的文件名和字节偏移转化为文件中的块索引。然后，客户端向主服务器程序发送包含文件名和块索引的请求。主服务器会回复相应的块句柄和副本位置。客户端使用文件名和块索引作为密钥缓存这些信息。\n然后，客户端向其中一个副本（很可能是最近的副本）发送请求。该请求指定了数据块句柄和该数据块内的字节范围。在缓存信息过期或文件重新打开之前，对同一数据块的进一步读取不再需要客户端与主服务器端之间的交互。事实上，客户端通常会在同一个请求中请求多个数据块，主服务器程序也可以包含紧随其后的数据块信息。这些额外的信息可以避免今后客户端与主服务器端之间的多次交互，而且几乎没有额外成本。\n2.5 分块大小 块大小是关键设计参数之一。我们选择了 64 MB，这比典型的文件系统块大小大得多。每个块副本都作为普通 Linux 文件存储在块服务器上，并且仅在需要时进行扩展。懒惰空间分配可避免因内部碎片而浪费空间，这可能是对如此大的块大小的最大反对意见。\n大块大小具有几个重要优势。首先，它减少了客户端与主服务器程序交互的需要，因为对同一分块的读写只需向主服务器程序发出一次初始请求，以获取分块位置信息。这对我们的工作负载来说意义尤其重大，因为应用程序大多是顺序读写大文件。即使是小规模的随机读取，客户端也能轻松缓存多 TB 工作集的所有分块定位信息。其次，由于在大块上，客户端更有可能对给定的大块执行许多操作，因此可以通过长时间保持与大块服务器的持久 TCP 连接来减少网络开销。第三，它可以减少存储在主服务器上的元数据的大小。这样我们就能将元数据保存在内存中，从而带来其他优势，我们将在第 2.6.1 节中讨论。\n另一方面，即使使用懒惰空间分配，大块大小也有其缺点。小文件由少量的块组成，也许只有一个。如果有很多客户端访问同一个文件，存储这些分块的分块服务器可能会成为热点。在实际应用中，热点并不是一个大问题，因为我们的应用程序大多是按顺序读取大型多块文件。\n然而，当 GFS 首次用于批处理队列系统时，确实出现了热点：一个可执行文件以单块文件的形式写入 GFS，然后在数百台机器上同时启动。数以百计的同时请求使存储该可执行文件的少数主块服务器不堪重负。我们通过使用更高的复制系数来存储此类可执行文件，并让批队列系统错开应用程序的启动时间，从而解决了这一问题。一个潜在的长期解决方案是允许客户端在这种情况下从其他客户端读取数据。\n2.6 元数据 主服务器存储三种主要类型的元数据：文件和块命名空间、从文件到块的映射以及每个块的副本的位置。所有元数据都保存在主服务器的内存中。前两种类型（命名空间和文件到块的映射）也通过将变化记录到存储在主服务器本地磁盘上的操作日志中并复制到远程机器上来保持持久性。使用日志使我们能够简单、可靠地更新主服务器状态，并且在主服务器崩溃时不会冒不一致的风险。主服务器不会持久存储块位置信息。相反，它会在主服务器启动时以及每当有块服务器加入集群时向每个块服务器询问其块。\n2.6.1 内存数据结构 由于元数据存储在内存中，因此主站操作速度很快。此外，主站在后台定期扫描其整个状态也非常简单高效。这种周期性扫描被用来实现垃圾块收集、在主块服务器出现故障时进行重新复制以及主块迁移，以平衡主块服务器之间的负载和磁盘空间使用。第4.3节和第4.4节将进一步讨论这些活动。\n这种只使用内存的方法可能存在的一个问题是，分块的数量以及整个系统的容量都受到主服务器内存容量的限制。实际上，这并不是一个严重的限制。主文件会为每个 64 MB 的数据块维护少于 64 字节的元数据。大多数分块都是满的，因为大多数文件包含许多分块，只有最后一个分块可能被部分填满。同样，文件命名空间数据通常每个文件只需要少于 64 字节，因为它使用前缀压缩紧凑地存储了文件名。\n如果有必要支持更大的文件系统，为主文件系统增加额外内存的成本并不高，但却能通过在内存中存储元数据而获得简单性、可靠性、性能和灵活性。\n2.6.2 分块位置 主服务器程序不会持续记录哪些分块服务器拥有某个分块的副本。它只需在启动时轮询各块服务器以获取该信息。由于主服务器控制着所有的分块放置，并通过定期的 HeartBeat 信息监控分块服务器的状态，因此主服务器可以随时更新自己的信息。\n我们最初尝试在主服务器端持久保存块位置信息，但后来发现，在启动时和之后定期从块服务器请求数据要简单得多。这样就解决了主服务器和块服务器在块服务器加入、离开集群、更名、故障、重启等情况下保持同步的问题。在拥有数百台服务器的集群中，这些事件经常发生。\n理解这一设计决定的另一种方法是认识到，对于自己的磁盘上是否有数据块，数据块服务器拥有最终决定权。试图在主服务器上保持对这些信息的一致看法是没有意义的，因为数据块服务器上的错误可能会导致数据块自发消失（例如，磁盘坏掉并被禁用），或者操作员可能会重命名数据块服务器。\n2.6.3 操作日志 操作日志包含关键元数据更改的历史记录。它是 GFS 的核心。它不仅是元数据的唯一持久记录，还是定义并发操作顺序的逻辑时间线。文件和数据块以及它们的版本（见第 4.5 节）都以它们创建的逻辑时间为唯一且永恒的标识。\n由于操作日志至关重要，因此我们必须可靠地存储操作日志，并且在元数据更改持久化之前，不能让客户端看到更改。否则，即使数据块本身存活下来，我们也会丢失整个文件系统或最近的客户端操作。因此，我们将日志复制到多台远程机器上，只有在本地和远程将相应的日志记录刷新到磁盘后，才能响应客户端操作。在刷新之前，主服务器程序会将多条日志记录集中在一起，从而减少刷新和复制对整个系统吞吐量的影响。\n主服务器通过重放操作日志来恢复其文件系统状态。为了最大限度地缩短启动时间，我们必须保持日志较小。每当日志超过一定大小时，主服务器都会检查其状态，以便它可以通过从本地磁盘加载最新的检查点并在此之后仅重放有限数量的日志记录来恢复。检查点采用紧凑的 B 树形式，可以直接映射到内存中并用于命名空间查找，而无需额外解析。这进一步加快了恢复速度并提高了可用性。\n由于创建一个检查点可能需要一段时间，因此主站的内部状态结构应能在不耽误接收传入的变更的情况下创建新的检查点。主服务器会切换到新的日志文件，并在单独的线程中创建新的检查点。新的检查点包括切换前的所有变更。对于拥有几百万个文件的集群来说，一分钟左右就能创建完毕。完成后，它将被写入本地和远程磁盘。\n恢复只需要最新的完整检查点和后续日志文件。较早的检查点和日志文件可以随意删除，但我们会保留一些以防万一。检查点过程中的故障不会影响正确性，因为恢复代码会检测并跳过不完整的检查点。\n2.7 一致性模型 GFS 有一个宽松的一致性模型，可以很好地支持我们的高度分布式应用，而且实现起来相对简单高效。我们现在讨论 GFS 的保证及其对应用程序的意义。我们还将重点介绍 GFS 如何维护这些保证，但具体细节将留待本文其他部分讨论。\n2.7.1 GFS的保证 文件命名空间的变更（如文件创建）是原子性的。它们完全由主服务器程序处理：命名空间锁定保证了原子性和正确性（第 4.1 节）；主服务器程序的操作日志定义了这些操作的全局总顺序（第 2.6.3 节）。\n文件区域在数据变更后的状态取决于变更的类型、变更是否成功，以及是否存在并发变更。表1总结了结果。如果所有客户端无论从哪个副本读取数据都始终看到相同的数据，则文件区域是一致的。在文件数据变更后，如果区域是一致的，并且客户端能够完整地看到变更所写入的内容，则该区域被定义为一致区域。当一个变更在没有并发写入者干扰的情况下成功时，受影响的区域被定义为一致区域（暗示其一致性）：所有客户端将始终看到变更所写入的内容。并发成功的变更使区域未定义但仍然一致：所有客户端看到相同的数据，但这些数据可能无法反映任何单个变更的内容。通常，这些数据由多个变更的混合片段组成。失败的变更使区域变得不一致（因此也未定义）：不同的客户端可能在不同的时间看到不同的数据。下面我们将描述我们的应用程序如何区分定义区域和未定义区域。应用程序不需要进一步区分不同类型的未定义区域。\n数据变更可以是写入操作或记录追加操作。写入操作会在应用程序指定的文件偏移位置写入数据。记录追加操作会在存在并发变更的情况下，以原子方式至少追加一次数据（即“记录”），但写入的位置由 GFS 选择（见第3.3节）。(与此相反，“常规 “追加只是在客户端认为是当前文件末尾的偏移位置进行写入）。偏移量会返回给客户端，并标志着包含记录的定义区域的开始。此外，GFS 还可能在中间插入填充或重复记录。它们占据的区域被认为是不一致的，与用户数据量相比通常相形见绌。\n在一连串成功的变更之后，变更后的文件区域保证是已定义的，并包含最后一次变更所写入的数据。GFS 通过以下方式实现这一目标：(a) 在所有副本上以相同顺序对一个主块进行变异（第 3.1 节）；(b) 使用主块版本号来检测因其主块服务器宕机而错过变异的副本（第 4.5 节）。陈旧副本绝不会参与变更，也不会提供给向主服务器询问主块位置的客户端。它们会尽早被垃圾回收。\n由于客户端会缓存块定位，因此可能会在信息刷新前从陈旧的副本中读取信息。这个窗口会受到缓存条目超时和下一次打开文件的限制，因为下一次打开文件会从缓存中清除该文件的所有分块信息。此外，由于我们的大多数文件都是仅附加的，因此陈旧的副本通常会返回一个过早结束的分块，而不是过时的数据。当阅读器重试并联系主文件时，它将立即获得当前的分块位置。\n当然，在成功变更后的很长一段时间内，组件故障仍有可能损坏或毁坏数据。GFS 通过主服务器与所有主服务器之间的定期握手来识别故障的主服务器，并通过校验和来检测数据损坏（第 5.2 节）。一旦出现问题，就会尽快从有效副本中恢复数据（第 4.3 节）。只有当数据块的所有副本在 GFS 作出反应前丢失（通常在几分钟内），数据块才会不可逆转地丢失。即使在这种情况下，数据块也是不可用的，而不是损坏的：应用程序收到的是明确的错误，而不是损坏的数据。\n2.7.2 对应用的影响 GFS 应用程序可以通过其他用途所需的一些简单技术来适应宽松的一致性模型：依靠追加而不是覆盖、检查点以及写入自验证、自识别记录。\n实际上，我们所有的应用程序都是通过追加而不是覆盖来改变文件的。在一个典型的应用中，写入器从头到尾生成一个文件。在写入所有数据后，它会以原子方式将文件重命名为永久名称，或定期检查已成功写入多少数据。检查点还可能包括应用级校验和。读取器只验证和处理上一个检查点之前的文件区域，因为已知该区域处于定义的状态。无论一致性和并发性问题如何，这种方法都很有效。与随机写入相比，追加的效率要高得多，对应用程序故障的恢复能力也更强。检查点允许写入程序以增量方式重新启动，并防止读取程序处理从应用程序角度来看仍不完整的已成功写入文件数据。\n在另一种典型用途中，许多写入器同时向文件追加合并结果或作为生产者-消费者队列。记录追加的 “至少追加一次 “语义保留了每个写入器的输出。读取器会按如下方式处理偶尔出现的填充和重复。写入器编写的每条记录都包含校验和等额外信息，以便验证其有效性。阅读器可以使用校验和来识别和丢弃额外的填充和记录片段。如果不能容忍偶尔出现的重复记录（例如，如果重复记录会触发非幂等操作），则可以使用记录中的唯一标识符将其过滤掉。记录 I/O 的这些功能（除重复删除外）都在我们应用程序共享的库代码中，也适用于谷歌的其他文件接口实现。有了这些功能，我们就能始终向记录阅读器提供相同序列的记录，以及极少数的重复记录。\n3. 系统交互 我们在设计系统时尽量减少主服务器对所有操作的参与。有了上述背景，我们现在来介绍客户端、主服务器端和分块服务器是如何交互实现数据变更、原子记录追加和快照的。\n3.1 租约和变更顺序 变更是指更改数据块内容或元数据的操作，例如写入或追加操作。每次变更都会在所有数据块的副本上执行。我们使用租约来维护副本之间一致的变更顺序。主服务器将一个数据块的租约授予其中一个副本，我们称之为主副本。主副本为所有对该数据块的变更选择一个串行顺序。所有副本在应用变更时都遵循这个顺序。因此，全局变更顺序首先由主服务器选择的租约授予顺序定义，而在一个租约内，则由主副本分配的序列号来定义。\n租约机制旨在最小化主服务器的管理开销。租约的初始超时时间为60秒。然而，只要数据块正在被变更，主副本可以向主服务器请求并通常能够获得无限期的租约延长。这些延长请求和授予通过主服务器与所有数据块服务器定期交换的 HeartBeat 消息进行捆绑。当主服务器希望在文件重命名时禁用对该文件的变更时，有时会尝试在租约到期前撤销租约。即使主服务器与主副本失去通信，在旧租约到期后，它也可以安全地将新租约授予另一个副本。\n在图 2 中，我们按照写入的控制流，通过这些编号步骤来说明这一过程。\n客户端向主服务器询问当前哪个数据块服务器持有该数据块的租约，以及其他副本的位置。如果没有副本持有租约，主服务器将向它选择的一个副本授予租约（未显示）。 主服务器会回复主副本的身份和其他（次级）副本的位置。客户端将这些数据缓存以供将来的变更使用。只有在主副本变得不可达或回复称不再持有租约时，客户端才需要再次联系主服务器。 客户端将数据推送到所有副本。客户端可以按照任意顺序进行此操作。每个数据块服务器会在内部的LRU缓存中存储数据，直到数据被使用或过期。通过将数据流与控制流解耦，我们可以根据网络拓扑优化昂贵的数据流调度，而不依赖于哪个数据块服务器是主副本。第3.2节对此进行了进一步讨论。 一旦所有副本确认接收到数据，客户端会向主副本发送写请求。该请求标识了之前推送给所有副本的数据。主副本为接收到的所有变更（可能来自多个客户端）分配连续的序列号，以提供必要的序列化。它按照序列号的顺序将变更应用到其本地状态中。 主副本将写入请求转发给所有次级副本。每个次级副本按照主副本分配的序列号顺序应用变更。 所有次级副本都会回复主副本，表示它们已完成操作。 主副本会向客户端回复。任何在副本处遇到的错误都会报告给客户端。在发生错误的情况下，写操作可能在主副本成功，而在任意数量的次级副本中失败。（如果主副本的写操作失败，它将不会被分配序列号并进行转发。）客户端请求被视为失败，修改区域将处于不一致状态。我们的客户端代码通过重试失败的变更来处理这些错误。它会在步骤（3）到（7）之间进行几次尝试，然后才会回退到从写操作开始重新尝试。 如果应用程序的写操作较大或跨越了数据块边界，GFS客户端代码会将其分解为多个写操作。所有操作都会遵循上述控制流程，但可能与其他客户端的并发操作交错或被覆盖。因此，虽然副本之间的内容会保持一致（因为所有操作在所有副本上都以相同的顺序成功完成），共享文件区域可能最终包含来自不同客户端的片段。正如第2.7节提到的，这会使文件区域处于一致但未定义的状态。\n3.2 数据流 我们将数据流与控制流解耦，以更高效地利用网络资源。控制流从客户端传递到主副本，再从主副本传递到所有次级副本；而数据则以流水线的方式沿着精心选择的链式数据块服务器线性推送。我们的目标是充分利用每台机器的网络带宽，避免网络瓶颈和高延迟连接，并尽量减少推送全部数据的延迟。\n为了充分利用每台机器的网络带宽，数据被沿着一条数据块服务器链线性推送，而不是通过其他拓扑结构（例如树形结构）分发。这样，每台机器的全部出站带宽都用于尽可能快速地传输数据，而不是被分配给多个接收者。\n为了尽可能避免网络瓶颈和高延迟链路（例如，交换机间链路往往同时存在），每台机器都会将数据转发给网络拓扑结构中尚未接收数据的 “最近 的\"机器。假设客户端正在向分块服务器 S1 至 S4 推送数据。客户端将数据发送到最近的分块服务器，如 S1。S1 将数据转发给离 S1 最近的分块服务器 S2 至 S4，即 S2。同样，S2 将数据转发给离 S2 最近的 S3 或 S4，以此类推。我们的网络结构非常简单，“距离 “可以通过 IP 地址准确估算。\n为了最大限度地减少延迟，我们通过 TCP 连接进行数据传输的流水线化处理。一旦数据块服务器接收到一部分数据，它就会立即开始转发。流水线机制对我们特别有帮助，因为我们使用的是带有全双工链路的交换网络。立即发送数据不会降低接收速率。在没有网络拥塞的情况下，将 B 字节的数据传输到 R 个副本的理想时间为 B/T + RL，其中 T 是网络吞吐量，L 是两台机器之间传输数据的延迟。我们网络的典型链路速率为 100 Mbps (T)，而L远低于 1 毫秒。因此，1 MB 的数据理想情况下可以在大约 80ms 内分发完成。\n3.3 原子记录追加 GFS 提供了一种称为记录追加的原子追加操作。在传统的写操作中，客户端指定数据写入的偏移量。对于同一区域的并发写操作，无法实现可序列化：该区域可能最终包含来自多个客户端的数据片段。然而，在记录追加操作中，客户端只需指定数据。GFS 会以原子方式（即作为一个连续的字节序列）至少一次将数据追加到文件中，偏移量由 GFS 决定，并将该偏移量返回给客户端。这类似于在 Unix 系统中以 O_APPEND 模式打开文件进行写操作，但避免了多个写入者同时操作时的竞争条件。\n我们的分布式应用程序大量使用记录追加功能，其中不同机器上的许多客户端会同时追加到同一个文件。如果客户端采用传统的写入方式，则需要额外复杂而昂贵的同步，例如通过分布式锁管理器。在我们的工作负载中，此类文件通常用作多生产者/单消费者队列，或包含来自许多不同客户端的合并结果。\n记录追加是一种变更，它遵循第 3.1 节中的控制流，只在主副本有一些额外的逻辑。客户端将数据推送到文件最后一个分块的所有副本中，然后将请求发送给主副本。主副本会检查将记录追加到当前数据块是否会导致数据块超过最大容量（64 MB）。如果会，它就会将分块填充到最大大小，告诉辅助分块也这样做，并回复客户端，说明应在下一个分块上重试操作。(记录追加的大小最多只能是最大分块大小的四分之一，以便将最坏情况下的碎片保持在可接受的水平）。如果记录符合最大大小（这是常见的情况），主副本就会将数据追加到其副本中，并告诉次节点在其确切偏移量处写入数据，最后向客户端回复成功。\n如果任何副本的记录追加失败，客户端会重试操作。因此，同一分块的副本可能包含不同的数据，其中可能包括同一记录的全部或部分副本。GFS 并不保证所有副本在字节上完全相同。它只保证数据作为一个原子单元至少被写入一次。这一特性很容易从简单的观察中得出：要使操作报告成功，数据必须在某个数据块的所有副本上以相同的偏移量写入。此外，在此之后，所有副本的长度至少与记录末尾的长度相同，因此，即使后来不同的副本成为主副本，未来的记录也会被分配到更高的偏移量或不同的分块。就一致性保证而言，成功进行记录追加操作并写入数据的区域是确定的（因此是一致的），而中间区域则是不一致的（因此是未定义的）。正如我们在第 2.7.2 节中所讨论的，我们的应用程序可以处理不一致的区域。\n3.4 快照 快照操作几乎可以瞬间复制文件或目录树（“源”），同时最大限度地减少对正在进行的变更的干扰。我们的用户用它来快速创建庞大数据集的分支副本（通常是这些副本的递归副本），或者在尝试更改之前对当前状态进行检查，这些更改随后可以很容易地提交或回滚。\n与 AFS [5] 一样，我们使用标准的写时复制技术来实现快照。当主服务器接收到快照请求时，它首先会撤销即将快照文件中的数据块上的所有未完成的租约。这确保了对这些数据块的任何后续写入都需要与主服务器进行交互，以查找租约持有者。这将主服务器就有机会首先创建数据块的新副本。\n租约撤销或过期后，主服务器会将操作记录到磁盘上。然后，它通过复制源文件或目录树的元数据，将此日志记录应用到内存状态。新创建的快照文件指向与源文件相同的块。\n当客户端第一次想要在快照操作之后写入数据块 C 时，它会向主服务器发送请求以查找当前的租约持有者。主服务器注意到数据块 C 的引用计数大于1。它推迟了对客户端请求的回复，而是选择了一个新的数据块句柄 C’。然后，主服务器要求每个拥有 C 当前副本的数据块服务器创建一个名为 C’ 的新数据块。通过在与原始数据块相同的数据块服务器上创建新数据块，我们确保数据可以在本地复制，而不是通过网络复制（我们的磁盘速度约是100 Mb 以太网链接的三倍）。从这一点开始，请求处理与任何数据块的处理没有区别：主服务器将新的数据块 C’ 的租约授予其中一个副本，并回复客户端，客户端可以正常写入该数据块，而不知道它刚刚是从现有数据块创建的。\n4. 主服务器操作 主服务器执行所有命名空间操作。此外，它还负责管理整个系统中的分块副本：做出放置决定、创建新的分块和副本、协调各种全系统活动以保持分块完全复制、平衡所有分块服务器的负载以及回收未使用的存储空间。下面我们将逐一讨论这些主题。\n4.1 命名空间管理和锁定 许多主服务器操作可能需要较长时间：例如，快照操作必须撤销覆盖快照的所有数据块服务器上的租约。我们不希望在这些操作运行时延迟其他主服务器操作。因此，我们允许多个操作同时进行，并在命名空间的区域上使用锁来确保适当的序列化。\n与许多传统文件系统不同，GFS 没有每个目录的数据显示结构来列出该目录中的所有文件。它也不支持同一个文件或目录的别名（即 Unix 术语中的硬链接或符号链接）。GFS 在逻辑上将其命名空间表示为一个查找表，该表将完整路径名映射到元数据。通过前缀压缩，该表可以高效地在内存中表示。命名空间树中的每个节点（无论是绝对文件名还是绝对目录名）都有一个关联的读写锁。\n每个主操作都会在运行前获取一组锁。通常情况下，如果涉及 /d1/d2/.../dn/leaf，它将获得目录名 /d1、/d1/d2、…、/d1/d2/.../dn 的读锁，以及完整路径名 /d1/d2/.../dn/leaf 的读锁或写锁。请注意，根据操作的不同，leaf 可以是文件或目录。\n现在，我们将说明这种锁定机制如何防止 /home/user/foo 文件在 /home/user 被快照到 /save/user时被创建。快照操作会获取/home 和 /save 上的读锁，以及 /home/user 和 /save/user 上的写锁。文件创建会获取 /home 和 /home/user 上的读锁，以及 /home/user/foo 上的写锁。这两个操作将被正确序列化，因为它们试图获取 /home/user 上的冲突锁。文件创建不需要对父级目录加写锁，因为没有 “目录 “或类似于 inode 的数据结构需要防止修改。对名称的读取锁足以保护父目录不被删除。\n该锁机制的一个优点是它允许在同一目录中同时进行并发的修改。例如，可以在同一目录中同时执行多个文件创建操作：每个操作都会对目录名称获取读锁，并对文件名称获取写锁。对目录名称的读锁足以防止目录被删除、重命名或快照。而对文件名称的写锁则能序列化（排队）防止两次创建同名文件的尝试。\n由于命名空间可能有很多节点，因此读写锁对象的分配比较懒散，一旦不使用就会被删除。此外，为防止死锁，获取锁的总顺序是一致的：首先按命名空间树中的层级排序，然后在同一层级内按字典序排序。\n4.2 副本放置 一个 GFS 集群在多个层面上高度分布化。它通常拥有数百个分布在多个机架上的数据块服务器。这些数据块服务器又可能被来自同一机架或不同机架的数百个客户端访问。位于不同机架的两台机器之间的通信可能需要通过一个或多个网络交换机。此外，进入或离开某个机架的带宽可能小于该机架内所有机器的总带宽。多层级的分布化为数据分布带来了独特的挑战，涉及到可扩展性、可靠性和可用性的需求。\n分块副本放置策略有两个目的：最大化数据的可靠性和可用性，以及最大化网络带宽的利用。为此，仅将副本分布在不同机器上是不够的，这样只能防止磁盘或机器故障并充分利用每台机器的网络带宽。我们还必须将块副本分布在不同的机架上。这可以确保即使整个机架发生故障或下线（例如，由于共享资源故障，如网络交换机或电路问题），块的一些副本仍能存活并保持可用性。这也意味着某个块的流量，尤其是读操作，可以利用多个机架的总带宽。然而，写操作的流量需要通过多个机架，这是我们愿意接受的权衡。\n未完…\n","wordCount":"372","inLanguage":"en","image":"https://fireflyyh.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-10-24T00:00:00Z","dateModified":"2024-10-24T00:00:00Z","author":{"@type":"Person","name":"Jagger"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://fireflyyh.top/posts/distributionsystem/gfs%E7%BF%BB%E8%AF%91/"},"publisher":{"@type":"Organization","name":"Jagger's Blog","logo":{"@type":"ImageObject","url":"https://fireflyyh.top/img/yuan.jpg"}}}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y30PKHXBN4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y30PKHXBN4")</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://fireflyyh.top/ accesskey=h title="Jagger's Blog (Alt + H)"><img src=https://fireflyyh.top/img/laugh.jpg alt aria-label=logo height=35>Jagger's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://fireflyyh.top/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://fireflyyh.top/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://fireflyyh.top/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://fireflyyh.top/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://fireflyyh.top/about/ title=About><span>About</span></a></li><li><a href=https://github.com/starsYHyh/Jagger/ title=Github><span>Github</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://fireflyyh.top/>Home</a>&nbsp;»&nbsp;<a href=https://fireflyyh.top/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">The Google File System</h1><div class=post-meta><span title='2024-10-24 00:00:00 +0000 UTC'>October 24, 2024</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;372 words&nbsp;·&nbsp;Jagger&nbsp;|&nbsp;<a href=https://github.com/starsYHyh/Jagger/tree/main/content/posts/DistributionSystem/gfs%e7%bf%bb%e8%af%91.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e6%91%98%e8%a6%81 aria-label=摘要>摘要</a></li><li><a href=#1-%e5%bc%95%e8%a8%80 aria-label="1. 引言">1. 引言</a></li><li><a href=#2-%e8%ae%be%e8%ae%a1%e6%a6%82%e8%bf%b0 aria-label="2. 设计概述">2. 设计概述</a><ul><li><a href=#21-%e5%81%87%e8%ae%be aria-label="2.1 假设">2.1 假设</a></li><li><a href=#22-%e6%8e%a5%e5%8f%a3 aria-label="2.2 接口">2.2 接口</a></li><li><a href=#23-%e6%9e%b6%e6%9e%84 aria-label="2.3 架构">2.3 架构</a></li><li><a href=#24-%e5%8d%95%e4%b8%aa%e4%b8%bb%e6%9c%8d%e5%8a%a1%e5%99%a8 aria-label="2.4 单个主服务器">2.4 单个主服务器</a></li><li><a href=#25-%e5%88%86%e5%9d%97%e5%a4%a7%e5%b0%8f aria-label="2.5 分块大小">2.5 分块大小</a></li><li><a href=#26-%e5%85%83%e6%95%b0%e6%8d%ae aria-label="2.6 元数据">2.6 元数据</a><ul><li><a href=#261-%e5%86%85%e5%ad%98%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84 aria-label="2.6.1 内存数据结构">2.6.1 内存数据结构</a></li><li><a href=#262-%e5%88%86%e5%9d%97%e4%bd%8d%e7%bd%ae aria-label="2.6.2 分块位置">2.6.2 分块位置</a></li><li><a href=#263-%e6%93%8d%e4%bd%9c%e6%97%a5%e5%bf%97 aria-label="2.6.3 操作日志">2.6.3 操作日志</a></li></ul></li><li><a href=#27-%e4%b8%80%e8%87%b4%e6%80%a7%e6%a8%a1%e5%9e%8b aria-label="2.7 一致性模型">2.7 一致性模型</a><ul><li><a href=#271-gfs%e7%9a%84%e4%bf%9d%e8%af%81 aria-label="2.7.1 GFS的保证">2.7.1 GFS的保证</a></li><li><a href=#272-%e5%af%b9%e5%ba%94%e7%94%a8%e7%9a%84%e5%bd%b1%e5%93%8d aria-label="2.7.2 对应用的影响">2.7.2 对应用的影响</a></li></ul></li></ul></li><li><a href=#3-%e7%b3%bb%e7%bb%9f%e4%ba%a4%e4%ba%92 aria-label="3. 系统交互">3. 系统交互</a><ul><li><a href=#31-%e7%a7%9f%e7%ba%a6%e5%92%8c%e5%8f%98%e6%9b%b4%e9%a1%ba%e5%ba%8f aria-label="3.1 租约和变更顺序">3.1 租约和变更顺序</a></li><li><a href=#32-%e6%95%b0%e6%8d%ae%e6%b5%81 aria-label="3.2 数据流">3.2 数据流</a></li><li><a href=#33-%e5%8e%9f%e5%ad%90%e8%ae%b0%e5%bd%95%e8%bf%bd%e5%8a%a0 aria-label="3.3 原子记录追加">3.3 原子记录追加</a></li><li><a href=#34-%e5%bf%ab%e7%85%a7 aria-label="3.4 快照">3.4 快照</a></li></ul></li><li><a href=#4-%e4%b8%bb%e6%9c%8d%e5%8a%a1%e5%99%a8%e6%93%8d%e4%bd%9c aria-label="4. 主服务器操作">4. 主服务器操作</a><ul><li><a href=#41-%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4%e7%ae%a1%e7%90%86%e5%92%8c%e9%94%81%e5%ae%9a aria-label="4.1 命名空间管理和锁定">4.1 命名空间管理和锁定</a></li><li><a href=#42-%e5%89%af%e6%9c%ac%e6%94%be%e7%bd%ae aria-label="4.2 副本放置">4.2 副本放置</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=摘要>摘要<a hidden class=anchor aria-hidden=true href=#摘要>#</a></h2><p>我们设计并实现了谷歌文件系统，这是一个可扩展的分布式文件系统，适用于大型分布式数据密集型应用。该系统可在廉价的商品硬件上运行，同时提供容错功能，并能为大量客户端提供高聚合性能。</p><p>我们的设计目标与之前的分布式文件系统有许多相同之处，但我们对应用工作负载和技术环境（包括当前和预期环境）的观察结果表明，我们的设计明显偏离了之前的一些文件系统假设。这促使我们重新审视传统的选择，探索完全不同的设计要点。</p><p>文件系统成功地满足了我们的存储需求。它在谷歌内部被广泛部署，作为生成和处理我们的服务所使用的数据以及需要大型数据集的研发工作的存储平台。迄今为止，最大的集群在一千多台机器上的数千个磁盘上提供了数百 TB 的存储空间，并被数百个客户端并发访问。</p><p>在本文中，我们介绍了为支持分布式应用而设计的文件系统接口扩展，讨论了我们设计的许多方面，并报告了微基准测试和实际使用的测量结果。</p><h2 id=1-引言>1. 引言<a hidden class=anchor aria-hidden=true href=#1-引言>#</a></h2><p>我们设计并实施了谷歌文件系统（GFS），以满足谷歌快速增长的数据处理需求。GFS 与以前的分布式文件系统有许多相同的目标，如性能、可扩展性、可靠性和可用性。但是，在设计 GFS 时，我们对当前和预期的应用工作负载和技术环境进行了重要观察，这反映出我们明显偏离了之前的一些文件系统设计假设。我们重新审视了传统的选择，并探索了设计空间中完全不同的点。</p><p>首先，组件故障是常态而非例外。文件系统由数百甚至数千台存储机组成，这些存储机都是用廉价的商品部件制造的，并被数量相当的客户机访问。这些组件的数量和质量几乎可以保证，在任何特定时间都会有一些组件无法正常工作，而且有些组件无法从当前故障中恢复。我们见过应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源故障造成的问题。因此，持续监控、错误检测、容错和自动恢复必须成为系统的组成部分。</p><p>其次，按照传统的标准，文件是巨大的。多 GB 的文件很常见。每个文件通常包含许多应用对象，如网络文档。当我们经常处理由数十亿个对象组成的多 TB 快速增长的数据集时，即使文件系统可以支持，要管理数十亿个约 KB 大小的文件也很不方便。因此，必须重新审视 I/O 操作和块大小等设计假设和参数。</p><p>第三，大多数文件都是通过添加新数据而不是覆盖现有数据来改变的。文件内的随机写入几乎不存在。文件一旦写入，就只能读取，而且通常只能按顺序读取。各种数据都具有这些特征。有些可能是数据分析程序扫描过的大型存储库。有些可能是运行中的应用程序持续生成的数据流。有些可能是档案数据。有些可能是在一台机器上生成并在另一台机器上处理的中间结果，无论是同时处理还是稍后处理。鉴于巨型文件的这种访问模式，追加成为性能优化和原子性保证的重点，而在客户端缓存数据块则失去了吸引力。</p><p>第四，共同设计应用程序和文件系统 API 可以提高我们的灵活性，从而使整个系统受益。例如，我们放宽了 GFS 的一致性模型，大大简化了文件系统，而不会给应用程序带来沉重负担。我们还引入了原子追加操作，使多个客户端可以同时追加文件，而无需额外的同步。本文稍后将详细讨论这些内容。</p><p>目前，为不同目的部署了多个 GFS 集群。最大的集群有超过 1000 个存储节点，超过 300 TB 的磁盘存储空间，数百个客户端在不同的机器上持续大量访问。</p><h2 id=2-设计概述>2. 设计概述<a hidden class=anchor aria-hidden=true href=#2-设计概述>#</a></h2><h3 id=21-假设>2.1 假设<a hidden class=anchor aria-hidden=true href=#21-假设>#</a></h3><p>在设计满足我们需求的文件系统时，我们遵循的假设既是挑战也是机遇。我们在前面提到了一些重要的观察结果，现在详细介绍一下我们的假设。</p><ul><li>该系统由许多经常发生故障的廉价商品组件构成。系统必须不断进行自我监控，日常检测、容忍和及时恢复组件故障。</li><li>系统存储的大文件数量不多。我们预计会有几百万个文件，每个文件的大小通常为 100 MB 或更大。多 GB 文件是常见情况，应得到有效管理。我们必须支持小文件，但无需对其进行优化。</li><li>工作负载主要包括两种读取：大型流式读取和小型随机读取。在大数据流读取中，单个操作通常读取数百 KB，更常见的是 1 MB 或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小型随机读取通常在某个任意偏移位置读取几个 KB 的数据。注重性能的应用程序通常会对小规模读取进行批处理和排序，以稳定地读取文件，而不是来回读取。</li><li>这些工作负载中还有许多向文件追加数据的大型连续写入操作。典型的操作大小与读取类似。文件一旦写入，就很少再修改。支持在文件任意位置进行小规模写入，但不一定要高效。</li><li>系统必须有效地实现多个客户端同时追加到同一文件的定义明确的语义。我们的文件通常用作生产者-消费者队列或多路合并。数以百计的生产者（每台机器运行一个）将同时追加到一个文件。同步开销最小的原子性至关重要。文件可能会在稍后被读取，或者消费者可能会同时读取文件。</li><li>高持续带宽比低延迟更重要。我们的大多数目标应用都非常重视高速批量处理数据，而很少有应用对单个读取或写入的响应时间有严格要求。</li></ul><h3 id=22-接口>2.2 接口<a hidden class=anchor aria-hidden=true href=#22-接口>#</a></h3><p>GFS 提供了一个熟悉的文件系统接口，尽管它没有实现标准的 API（如 POSIX）。文件在目录中按层次组织，并用路径名标识。我们支持<strong>创建、删除、打开、关闭、读取</strong>和<strong>写入文件</strong>的常规操作。</p><p>此外，GFS 还有<strong>快照</strong>和<strong>记录追加</strong>操作。快照以低成本创建文件或目录树的副本。记录追加允许多个客户端同时向同一个文件追加数据，同时保证每个客户端追加的原子性。它适用于实现多向合并结果和生产者-消费者队列，许多客户端可以同时追加数据而无需额外锁定。我们发现，这些类型的文件在构建大型分布式应用时非常有用。快照追加和记录追加将分别在第 3.4 节和第 3.3 节中进一步讨论。</p><h3 id=23-架构>2.3 架构<a hidden class=anchor aria-hidden=true href=#23-架构>#</a></h3><p>如<strong>图 1</strong> 所示，GFS 集群由一个<strong>主服务器</strong>和多个<strong>分块服务器</strong>组成，并由多个<strong>客户端</strong>访问。每个客户端通常是一台运行用户级服务器进程的商用 Linux 机器。只要机器资源允许，并且可以接受运行可能不稳定的应用程序代码所带来的较低可靠性，在同一台机器上同时运行分块服务器和客户端是很容易的。</p><p><img loading=lazy src=https://picgo-01.oss-cn-shanghai.aliyuncs.com/Images/gfsFigure1.png alt="Figure 1: GFS Architecture"></p><p>文件被分成固定大小的<strong>块</strong>。每个分块都由主服务器程序在创建分块时分配的不可更改且全球唯一的 64 位<strong>分块句柄</strong>来标识。分块服务器将分块作为 Linux 文件存储在本地磁盘上，并读取或写入由分块句柄和字节范围指定的分块数据。为了保证可靠性，每个数据块都会在多个数据块服务器上复制。默认情况下，我们会存储三个副本，但用户可以为文件命名空间的不同区域指定不同的复制级别。</p><p>主文件系统维护所有文件系统元数据。其中包括命名空间、访问控制信息、文件到分块的映射以及分块的当前位置。主服务器还控制着全系统的活动，如块租约管理、孤儿块的垃圾回收以及块服务器之间的块迁移。主服务器会定期通过 <strong>HeartBeat</strong> 消息与每个块服务器进行通信，向其下达指令并收集其状态。</p><p>连接到每个应用程序的 GFS 客户端代码实现文件系统 API，并与主服务器程序和块服务器通信，代表应用程序读取或写入数据。客户端与主服务器交互元数据操作，但所有数据通信都直接与块服务器进行。我们不提供 POSIX API，因此无需连接 Linux vnode 层。</p><p>客户端和分块服务器都不缓存文件数据。客户端缓存的好处不大，因为大多数应用程序都会流式处理大量文件，或者工作集过大而无法缓存。没有缓存可以消除缓存一致性问题，从而简化客户端和整个系统。(但客户端会缓存元数据）。块服务器无需缓存文件数据，因为块是以本地文件的形式存储的，因此 Linux 的缓冲缓存已经将经常访问的数据保存在内存中。</p><h3 id=24-单个主服务器>2.4 单个主服务器<a hidden class=anchor aria-hidden=true href=#24-单个主服务器>#</a></h3><p>单个主站大大简化了我们的设计，并使主站能够利用全局知识做出复杂的分块放置和复制决策。不过，我们必须尽量减少主服务器程序对读写的参与，以免其成为瓶颈。客户端从不通过主服务器读写文件数据。相反，客户端会询问主服务器它应该联系哪些分块服务器。客户端会在有限的时间内缓存这些信息，并在随后的许多操作中直接与分块服务器交互。</p><p>让我们参照<strong>图 1</strong> 来解释一下简单读取的交互过程。首先，客户端使用固定的块大小，将应用程序指定的文件名和字节偏移转化为文件中的块索引。然后，客户端向主服务器程序发送包含文件名和块索引的请求。主服务器会回复相应的块句柄和副本位置。客户端使用文件名和块索引作为密钥缓存这些信息。</p><p>然后，客户端向其中一个副本（很可能是最近的副本）发送请求。该请求指定了数据块句柄和该数据块内的字节范围。在缓存信息过期或文件重新打开之前，对同一数据块的进一步读取不再需要客户端与主服务器端之间的交互。事实上，客户端通常会在同一个请求中请求多个数据块，主服务器程序也可以包含紧随其后的数据块信息。这些额外的信息可以避免今后客户端与主服务器端之间的多次交互，而且几乎没有额外成本。</p><h3 id=25-分块大小>2.5 分块大小<a hidden class=anchor aria-hidden=true href=#25-分块大小>#</a></h3><p>块大小是关键设计参数之一。我们选择了 64 MB，这比典型的文件系统块大小大得多。每个块副本都作为普通 Linux 文件存储在块服务器上，并且仅在需要时进行扩展。懒惰空间分配可避免因内部碎片而浪费空间，这可能是对如此大的块大小的最大反对意见。</p><p>大块大小具有几个重要优势。首先，它减少了客户端与主服务器程序交互的需要，因为对同一分块的读写只需向主服务器程序发出一次初始请求，以获取分块位置信息。这对我们的工作负载来说意义尤其重大，因为应用程序大多是顺序读写大文件。即使是小规模的随机读取，客户端也能轻松缓存多 TB 工作集的所有分块定位信息。其次，由于在大块上，客户端更有可能对给定的大块执行许多操作，因此可以通过长时间保持与大块服务器的持久 TCP 连接来减少网络开销。第三，它可以减少存储在主服务器上的元数据的大小。这样我们就能将元数据保存在内存中，从而带来其他优势，我们将在第 2.6.1 节中讨论。</p><p>另一方面，即使使用懒惰空间分配，大块大小也有其缺点。小文件由少量的块组成，也许只有一个。如果有很多客户端访问同一个文件，存储这些分块的分块服务器可能会成为热点。在实际应用中，热点并不是一个大问题，因为我们的应用程序大多是按顺序读取大型多块文件。</p><p>然而，当 GFS 首次用于批处理队列系统时，确实出现了热点：一个可执行文件以单块文件的形式写入 GFS，然后在数百台机器上同时启动。数以百计的同时请求使存储该可执行文件的少数主块服务器不堪重负。我们通过使用更高的复制系数来存储此类可执行文件，并让批队列系统错开应用程序的启动时间，从而解决了这一问题。一个潜在的长期解决方案是允许客户端在这种情况下从其他客户端读取数据。</p><h3 id=26-元数据>2.6 元数据<a hidden class=anchor aria-hidden=true href=#26-元数据>#</a></h3><p>主服务器存储三种主要类型的元数据：文件和块命名空间、从文件到块的映射以及每个块的副本的位置。所有元数据都保存在主服务器的内存中。前两种类型（命名空间和文件到块的映射）也通过将变化记录到存储在主服务器本地磁盘上的操作日志中并复制到远程机器上来保持持久性。使用日志使我们能够简单、可靠地更新主服务器状态，并且在主服务器崩溃时不会冒不一致的风险。主服务器不会持久存储块位置信息。相反，它会在主服务器启动时以及每当有块服务器加入集群时向每个块服务器询问其块。</p><h4 id=261-内存数据结构>2.6.1 内存数据结构<a hidden class=anchor aria-hidden=true href=#261-内存数据结构>#</a></h4><p>由于元数据存储在内存中，因此主站操作速度很快。此外，主站在后台定期扫描其整个状态也非常简单高效。这种周期性扫描被用来实现垃圾块收集、在主块服务器出现故障时进行重新复制以及主块迁移，以平衡主块服务器之间的负载和磁盘空间使用。第4.3节和第4.4节将进一步讨论这些活动。</p><p>这种只使用内存的方法可能存在的一个问题是，分块的数量以及整个系统的容量都受到主服务器内存容量的限制。实际上，这并不是一个严重的限制。主文件会为每个 64 MB 的数据块维护少于 64 字节的元数据。大多数分块都是满的，因为大多数文件包含许多分块，只有最后一个分块可能被部分填满。同样，文件命名空间数据通常每个文件只需要少于 64 字节，因为它使用前缀压缩紧凑地存储了文件名。</p><p>如果有必要支持更大的文件系统，为主文件系统增加额外内存的成本并不高，但却能通过在内存中存储元数据而获得简单性、可靠性、性能和灵活性。</p><h4 id=262-分块位置>2.6.2 分块位置<a hidden class=anchor aria-hidden=true href=#262-分块位置>#</a></h4><p>主服务器程序不会持续记录哪些分块服务器拥有某个分块的副本。它只需在启动时轮询各块服务器以获取该信息。由于主服务器控制着所有的分块放置，并通过定期的 <strong>HeartBeat</strong> 信息监控分块服务器的状态，因此主服务器可以随时更新自己的信息。</p><p>我们最初尝试在主服务器端持久保存块位置信息，但后来发现，在启动时和之后定期从块服务器请求数据要简单得多。这样就解决了主服务器和块服务器在块服务器加入、离开集群、更名、故障、重启等情况下保持同步的问题。在拥有数百台服务器的集群中，这些事件经常发生。</p><p>理解这一设计决定的另一种方法是认识到，对于自己的磁盘上是否有数据块，数据块服务器拥有最终决定权。试图在主服务器上保持对这些信息的一致看法是没有意义的，因为数据块服务器上的错误可能会导致数据块自发消失（例如，磁盘坏掉并被禁用），或者操作员可能会重命名数据块服务器。</p><h4 id=263-操作日志>2.6.3 操作日志<a hidden class=anchor aria-hidden=true href=#263-操作日志>#</a></h4><p>操作日志包含关键元数据更改的历史记录。它是 GFS 的核心。它不仅是元数据的唯一持久记录，还是定义并发操作顺序的逻辑时间线。文件和数据块以及它们的版本（见第 4.5 节）都以它们创建的逻辑时间为唯一且永恒的标识。</p><p>由于操作日志至关重要，因此我们必须可靠地存储操作日志，并且在元数据更改持久化之前，不能让客户端看到更改。否则，即使数据块本身存活下来，我们也会丢失整个文件系统或最近的客户端操作。因此，我们将日志复制到多台远程机器上，只有在本地和远程将相应的日志记录刷新到磁盘后，才能响应客户端操作。在刷新之前，主服务器程序会将多条日志记录集中在一起，从而减少刷新和复制对整个系统吞吐量的影响。</p><p>主服务器通过重放操作日志来恢复其文件系统状态。为了最大限度地缩短启动时间，我们必须保持日志较小。每当日志超过一定大小时，主服务器都会检查其状态，以便它可以通过从本地磁盘加载最新的检查点并在此之后仅重放有限数量的日志记录来恢复。检查点采用紧凑的 B 树形式，可以直接映射到内存中并用于命名空间查找，而无需额外解析。这进一步加快了恢复速度并提高了可用性。</p><p>由于创建一个检查点可能需要一段时间，因此主站的内部状态结构应能在不耽误接收传入的变更的情况下创建新的检查点。主服务器会切换到新的日志文件，并在单独的线程中创建新的检查点。新的检查点包括切换前的所有变更。对于拥有几百万个文件的集群来说，一分钟左右就能创建完毕。完成后，它将被写入本地和远程磁盘。</p><p>恢复只需要最新的完整检查点和后续日志文件。较早的检查点和日志文件可以随意删除，但我们会保留一些以防万一。检查点过程中的故障不会影响正确性，因为恢复代码会检测并跳过不完整的检查点。</p><h3 id=27-一致性模型>2.7 一致性模型<a hidden class=anchor aria-hidden=true href=#27-一致性模型>#</a></h3><p>GFS 有一个宽松的一致性模型，可以很好地支持我们的高度分布式应用，而且实现起来相对简单高效。我们现在讨论 GFS 的保证及其对应用程序的意义。我们还将重点介绍 GFS 如何维护这些保证，但具体细节将留待本文其他部分讨论。</p><h4 id=271-gfs的保证>2.7.1 GFS的保证<a hidden class=anchor aria-hidden=true href=#271-gfs的保证>#</a></h4><p>文件命名空间的变更（如文件创建）是原子性的。它们完全由主服务器程序处理：命名空间锁定保证了原子性和正确性（第 4.1 节）；主服务器程序的操作日志定义了这些操作的全局总顺序（第 2.6.3 节）。</p><p><img loading=lazy src=https://picgo-01.oss-cn-shanghai.aliyuncs.com/Images/gfsTable1.png alt="Table 1: File Region State After Mutation"></p><p>文件区域在数据变更后的状态取决于变更的类型、变更是否成功，以及是否存在并发变更。<strong>表1</strong>总结了结果。如果所有客户端无论从哪个副本读取数据都始终看到相同的数据，则文件区域是<strong>一致的</strong>。在文件数据变更后，如果区域是一致的，并且客户端能够完整地看到变更所写入的内容，则该区域被定义为<strong>一致区域</strong>。当一个变更在没有并发写入者干扰的情况下成功时，受影响的区域被定义为一致区域（暗示其一致性）：所有客户端将始终看到变更所写入的内容。并发成功的变更使区域未定义但仍然一致：所有客户端看到相同的数据，但这些数据可能无法反映任何单个变更的内容。通常，这些数据由多个变更的混合片段组成。失败的变更使区域变得不一致（因此也未定义）：不同的客户端可能在不同的时间看到不同的数据。下面我们将描述我们的应用程序如何区分定义区域和未定义区域。应用程序不需要进一步区分不同类型的未定义区域。</p><p>数据变更可以是<strong>写入</strong>操作或<strong>记录追加</strong>操作。写入操作会在应用程序指定的文件偏移位置写入数据。记录追加操作会在存在并发变更的情况下，以<strong>原子方式</strong>至少追加一次数据（即“记录”），但写入的位置由 GFS 选择（见第3.3节）。(与此相反，&ldquo;常规 &ldquo;追加只是在客户端认为是当前文件末尾的偏移位置进行写入）。偏移量会返回给客户端，并标志着包含记录的定义区域的开始。此外，GFS 还可能在中间插入填充或重复记录。它们占据的区域被认为是不一致的，与用户数据量相比通常相形见绌。</p><p>在一连串成功的变更之后，变更后的文件区域保证是已定义的，并包含最后一次变更所写入的数据。GFS 通过以下方式实现这一目标：(a) 在所有副本上以相同顺序对一个主块进行变异（第 3.1 节）；(b) 使用主块版本号来检测因其主块服务器宕机而错过变异的副本（第 4.5 节）。陈旧副本绝不会参与变更，也不会提供给向主服务器询问主块位置的客户端。它们会尽早被垃圾回收。</p><p>由于客户端会缓存块定位，因此可能会在信息刷新前从陈旧的副本中读取信息。这个窗口会受到缓存条目超时和下一次打开文件的限制，因为下一次打开文件会从缓存中清除该文件的所有分块信息。此外，由于我们的大多数文件都是仅附加的，因此陈旧的副本通常会返回一个过早结束的分块，而不是过时的数据。当阅读器重试并联系主文件时，它将立即获得当前的分块位置。</p><p>当然，在成功变更后的很长一段时间内，组件故障仍有可能损坏或毁坏数据。GFS 通过主服务器与所有主服务器之间的定期握手来识别故障的主服务器，并通过校验和来检测数据损坏（第 5.2 节）。一旦出现问题，就会尽快从有效副本中恢复数据（第 4.3 节）。只有当数据块的所有副本在 GFS 作出反应前丢失（通常在几分钟内），数据块才会不可逆转地丢失。即使在这种情况下，数据块也是不可用的，而不是损坏的：应用程序收到的是明确的错误，而不是损坏的数据。</p><h4 id=272-对应用的影响>2.7.2 对应用的影响<a hidden class=anchor aria-hidden=true href=#272-对应用的影响>#</a></h4><p>GFS 应用程序可以通过其他用途所需的一些简单技术来适应宽松的一致性模型：依靠追加而不是覆盖、检查点以及写入自验证、自识别记录。</p><p>实际上，我们所有的应用程序都是通过追加而不是覆盖来改变文件的。在一个典型的应用中，写入器从头到尾生成一个文件。在写入所有数据后，它会以原子方式将文件重命名为永久名称，或定期检查已成功写入多少数据。检查点还可能包括应用级校验和。读取器只验证和处理上一个检查点之前的文件区域，因为已知该区域处于定义的状态。无论一致性和并发性问题如何，这种方法都很有效。与随机写入相比，追加的效率要高得多，对应用程序故障的恢复能力也更强。检查点允许写入程序以增量方式重新启动，并防止读取程序处理从应用程序角度来看仍不完整的已成功写入文件数据。</p><p>在另一种典型用途中，许多写入器同时向文件追加合并结果或作为生产者-消费者队列。记录追加的 &ldquo;至少追加一次 &ldquo;语义保留了每个写入器的输出。读取器会按如下方式处理偶尔出现的填充和重复。写入器编写的每条记录都包含校验和等额外信息，以便验证其有效性。阅读器可以使用校验和来识别和丢弃额外的填充和记录片段。如果不能容忍偶尔出现的重复记录（例如，如果重复记录会触发非幂等操作），则可以使用记录中的唯一标识符将其过滤掉。记录 I/O 的这些功能（除重复删除外）都在我们应用程序共享的库代码中，也适用于谷歌的其他文件接口实现。有了这些功能，我们就能始终向记录阅读器提供相同序列的记录，以及极少数的重复记录。</p><h2 id=3-系统交互>3. 系统交互<a hidden class=anchor aria-hidden=true href=#3-系统交互>#</a></h2><p>我们在设计系统时尽量减少主服务器对所有操作的参与。有了上述背景，我们现在来介绍客户端、主服务器端和分块服务器是如何交互实现数据变更、原子记录追加和快照的。</p><h3 id=31-租约和变更顺序>3.1 租约和变更顺序<a hidden class=anchor aria-hidden=true href=#31-租约和变更顺序>#</a></h3><p>变更是指更改数据块内容或元数据的操作，例如写入或追加操作。每次变更都会在所有数据块的副本上执行。我们使用租约来维护副本之间一致的变更顺序。主服务器将一个数据块的租约授予其中一个副本，我们称之为<strong>主副本</strong>。主副本为所有对该数据块的变更选择一个串行顺序。所有副本在应用变更时都遵循这个顺序。因此，全局变更顺序首先由主服务器选择的租约授予顺序定义，而在一个租约内，则由主副本分配的序列号来定义。</p><p>租约机制旨在最小化主服务器的管理开销。租约的初始超时时间为60秒。然而，只要数据块正在被变更，主副本可以向主服务器请求并通常能够获得无限期的租约延长。这些延长请求和授予通过主服务器与所有数据块服务器定期交换的 HeartBeat 消息进行捆绑。当主服务器希望在文件重命名时禁用对该文件的变更时，有时会尝试在租约到期前撤销租约。即使主服务器与主副本失去通信，在旧租约到期后，它也可以安全地将新租约授予另一个副本。</p><p><img loading=lazy src=https://picgo-01.oss-cn-shanghai.aliyuncs.com/Images/gfsFigure2.png alt="Figure 2: Wirte Control and Data Flow"></p><p>在<strong>图 2</strong> 中，我们按照写入的控制流，通过这些编号步骤来说明这一过程。</p><ol><li>客户端向主服务器询问当前哪个数据块服务器持有该数据块的租约，以及其他副本的位置。如果没有副本持有租约，主服务器将向它选择的一个副本授予租约（未显示）。</li><li>主服务器会回复主副本的身份和其他（次级）副本的位置。客户端将这些数据缓存以供将来的变更使用。只有在主副本变得不可达或回复称不再持有租约时，客户端才需要再次联系主服务器。</li><li>客户端将数据推送到所有副本。客户端可以按照任意顺序进行此操作。每个数据块服务器会在内部的LRU缓存中存储数据，直到数据被使用或过期。通过将数据流与控制流解耦，我们可以根据网络拓扑优化昂贵的数据流调度，而不依赖于哪个数据块服务器是主副本。第3.2节对此进行了进一步讨论。</li><li>一旦所有副本确认接收到数据，客户端会向主副本发送写请求。该请求标识了之前推送给所有副本的数据。主副本为接收到的所有变更（可能来自多个客户端）分配连续的序列号，以提供必要的序列化。它按照序列号的顺序将变更应用到其本地状态中。</li><li>主副本将写入请求转发给所有次级副本。每个次级副本按照主副本分配的序列号顺序应用变更。</li><li>所有次级副本都会回复主副本，表示它们已完成操作。</li><li>主副本会向客户端回复。任何在副本处遇到的错误都会报告给客户端。在发生错误的情况下，写操作可能在主副本成功，而在任意数量的次级副本中失败。（如果主副本的写操作失败，它将不会被分配序列号并进行转发。）客户端请求被视为失败，修改区域将处于不一致状态。我们的客户端代码通过重试失败的变更来处理这些错误。它会在步骤（3）到（7）之间进行几次尝试，然后才会回退到从写操作开始重新尝试。</li></ol><p>如果应用程序的写操作较大或跨越了数据块边界，GFS客户端代码会将其分解为多个写操作。所有操作都会遵循上述控制流程，但可能与其他客户端的并发操作交错或被覆盖。因此，虽然副本之间的内容会保持一致（因为所有操作在所有副本上都以相同的顺序成功完成），共享文件区域可能最终包含来自不同客户端的片段。正如第2.7节提到的，这会使文件区域处于<strong>一致但未定义</strong>的状态。</p><h3 id=32-数据流>3.2 数据流<a hidden class=anchor aria-hidden=true href=#32-数据流>#</a></h3><p>我们将数据流与控制流解耦，以更高效地利用网络资源。控制流从客户端传递到主副本，再从主副本传递到所有次级副本；而数据则以流水线的方式沿着精心选择的链式数据块服务器线性推送。我们的目标是充分利用每台机器的网络带宽，避免网络瓶颈和高延迟连接，并尽量减少推送全部数据的延迟。</p><p>为了充分利用每台机器的网络带宽，数据被沿着一条数据块服务器链线性推送，而不是通过其他拓扑结构（例如树形结构）分发。这样，每台机器的全部出站带宽都用于尽可能快速地传输数据，而不是被分配给多个接收者。</p><p>为了尽可能避免网络瓶颈和高延迟链路（例如，交换机间链路往往同时存在），每台机器都会将数据转发给网络拓扑结构中尚未接收数据的 &ldquo;最近 的"机器。假设客户端正在向分块服务器 S1 至 S4 推送数据。客户端将数据发送到最近的分块服务器，如 S1。S1 将数据转发给离 S1 最近的分块服务器 S2 至 S4，即 S2。同样，S2 将数据转发给离 S2 最近的 S3 或 S4，以此类推。我们的网络结构非常简单，&ldquo;距离 &ldquo;可以通过 IP 地址准确估算。</p><p>为了最大限度地减少延迟，我们通过 TCP 连接进行数据传输的流水线化处理。一旦数据块服务器接收到一部分数据，它就会立即开始转发。流水线机制对我们特别有帮助，因为我们使用的是带有全双工链路的交换网络。立即发送数据不会降低接收速率。在没有网络拥塞的情况下，将 B 字节的数据传输到 R 个副本的理想时间为 B/T + RL，其中 T 是网络吞吐量，L 是两台机器之间传输数据的延迟。我们网络的典型链路速率为 100 Mbps (T)，而L远低于 1 毫秒。因此，1 MB 的数据理想情况下可以在大约 80ms 内分发完成。</p><h3 id=33-原子记录追加>3.3 原子记录追加<a hidden class=anchor aria-hidden=true href=#33-原子记录追加>#</a></h3><p>GFS 提供了一种称为<strong>记录追加</strong>的原子追加操作。在传统的写操作中，客户端指定数据写入的偏移量。对于同一区域的并发写操作，无法实现可序列化：该区域可能最终包含来自多个客户端的数据片段。然而，在记录追加操作中，客户端只需指定数据。GFS 会以原子方式（即作为一个连续的字节序列）至少一次将数据追加到文件中，偏移量由 GFS 决定，并将该偏移量返回给客户端。这类似于在 Unix 系统中以 <code>O_APPEND</code> 模式打开文件进行写操作，但避免了多个写入者同时操作时的竞争条件。</p><p>我们的分布式应用程序大量使用记录追加功能，其中不同机器上的许多客户端会同时追加到同一个文件。如果客户端采用传统的写入方式，则需要额外复杂而昂贵的同步，例如通过分布式锁管理器。在我们的工作负载中，此类文件通常用作多生产者/单消费者队列，或包含来自许多不同客户端的合并结果。</p><p>记录追加是一种变更，它遵循第 3.1 节中的控制流，只在主副本有一些额外的逻辑。客户端将数据推送到文件最后一个分块的所有副本中，然后将请求发送给主副本。主副本会检查将记录追加到当前数据块是否会导致数据块超过最大容量（64 MB）。如果会，它就会将分块填充到最大大小，告诉辅助分块也这样做，并回复客户端，说明应在下一个分块上重试操作。(记录追加的大小最多只能是最大分块大小的四分之一，以便将最坏情况下的碎片保持在可接受的水平）。如果记录符合最大大小（这是常见的情况），主副本就会将数据追加到其副本中，并告诉次节点在其确切偏移量处写入数据，最后向客户端回复成功。</p><p>如果任何副本的记录追加失败，客户端会重试操作。因此，同一分块的副本可能包含不同的数据，其中可能包括同一记录的全部或部分副本。GFS 并不保证所有副本在字节上完全相同。它只保证数据作为一个原子单元至少被写入一次。这一特性很容易从简单的观察中得出：要使操作报告成功，数据必须在某个数据块的所有副本上以相同的偏移量写入。此外，在此之后，所有副本的长度至少与记录末尾的长度相同，因此，即使后来不同的副本成为主副本，未来的记录也会被分配到更高的偏移量或不同的分块。就一致性保证而言，成功进行记录追加操作并写入数据的区域是确定的（因此是一致的），而中间区域则是不一致的（因此是未定义的）。正如我们在第 2.7.2 节中所讨论的，我们的应用程序可以处理不一致的区域。</p><h3 id=34-快照>3.4 快照<a hidden class=anchor aria-hidden=true href=#34-快照>#</a></h3><p>快照操作几乎可以瞬间复制文件或目录树（&ldquo;源&rdquo;），同时最大限度地减少对正在进行的变更的干扰。我们的用户用它来快速创建庞大数据集的分支副本（通常是这些副本的递归副本），或者在尝试更改之前对当前状态进行检查，这些更改随后可以很容易地提交或回滚。</p><p>与 AFS [5] 一样，我们使用标准的写时复制技术来实现快照。当主服务器接收到快照请求时，它首先会撤销即将快照文件中的数据块上的所有未完成的租约。这确保了对这些数据块的任何后续写入都需要与主服务器进行交互，以查找租约持有者。这将主服务器就有机会首先创建数据块的新副本。</p><p>租约撤销或过期后，主服务器会将操作记录到磁盘上。然后，它通过复制源文件或目录树的元数据，将此日志记录应用到内存状态。新创建的快照文件指向与源文件相同的块。</p><p>当客户端第一次想要在快照操作之后写入数据块 C 时，它会向主服务器发送请求以查找当前的租约持有者。主服务器注意到数据块 C 的引用计数大于1。它推迟了对客户端请求的回复，而是选择了一个新的数据块句柄 C&rsquo;。然后，主服务器要求每个拥有 C 当前副本的数据块服务器创建一个名为 C&rsquo; 的新数据块。通过在与原始数据块相同的数据块服务器上创建新数据块，我们确保数据可以在本地复制，而不是通过网络复制（我们的磁盘速度约是100 Mb 以太网链接的三倍）。从这一点开始，请求处理与任何数据块的处理没有区别：主服务器将新的数据块 C&rsquo; 的租约授予其中一个副本，并回复客户端，客户端可以正常写入该数据块，而不知道它刚刚是从现有数据块创建的。</p><h2 id=4-主服务器操作>4. 主服务器操作<a hidden class=anchor aria-hidden=true href=#4-主服务器操作>#</a></h2><p>主服务器执行所有命名空间操作。此外，它还负责管理整个系统中的分块副本：做出放置决定、创建新的分块和副本、协调各种全系统活动以保持分块完全复制、平衡所有分块服务器的负载以及回收未使用的存储空间。下面我们将逐一讨论这些主题。</p><h3 id=41-命名空间管理和锁定>4.1 命名空间管理和锁定<a hidden class=anchor aria-hidden=true href=#41-命名空间管理和锁定>#</a></h3><p>许多主服务器操作可能需要较长时间：例如，快照操作必须撤销覆盖快照的所有数据块服务器上的租约。我们不希望在这些操作运行时延迟其他主服务器操作。因此，我们允许多个操作同时进行，并在命名空间的区域上使用锁来确保适当的序列化。</p><p>与许多传统文件系统不同，GFS 没有每个目录的数据显示结构来列出该目录中的所有文件。它也不支持同一个文件或目录的别名（即 Unix 术语中的硬链接或符号链接）。GFS 在逻辑上将其命名空间表示为一个查找表，该表将完整路径名映射到元数据。通过前缀压缩，该表可以高效地在内存中表示。命名空间树中的每个节点（无论是绝对文件名还是绝对目录名）都有一个关联的读写锁。</p><p>每个主操作都会在运行前获取一组锁。通常情况下，如果涉及 <code>/d1/d2/.../dn/leaf</code>，它将获得目录名 <code>/d1</code>、<code>/d1/d2</code>、&mldr;、<code>/d1/d2/.../dn</code> 的读锁，以及完整路径名 <code>/d1/d2/.../dn/leaf</code> 的读锁或写锁。请注意，根据操作的不同，<code>leaf</code> 可以是文件或目录。</p><p>现在，我们将说明这种锁定机制如何防止 <code>/home/user/foo</code> 文件在 <code>/home/user</code> 被快照到 <code>/save/user</code>时被创建。快照操作会获取<code>/home</code> 和 <code>/save</code> 上的读锁，以及 <code>/home/user</code> 和 <code>/save/user</code> 上的写锁。文件创建会获取 <code>/home</code> 和 <code>/home/user</code> 上的读锁，以及 <code>/home/user/foo</code> 上的写锁。这两个操作将被正确序列化，因为它们试图获取 <code>/home/user</code> 上的冲突锁。文件创建不需要对父级目录加写锁，因为没有 &ldquo;目录 &ldquo;或类似于 inode 的数据结构需要防止修改。对名称的读取锁足以保护父目录不被删除。</p><p>该锁机制的一个优点是它允许在同一目录中同时进行并发的修改。例如，可以在同一目录中同时执行多个文件创建操作：每个操作都会对目录名称获取读锁，并对文件名称获取写锁。对目录名称的读锁足以防止目录被删除、重命名或快照。而对文件名称的写锁则能序列化（排队）防止两次创建同名文件的尝试。</p><p>由于命名空间可能有很多节点，因此读写锁对象的分配比较懒散，一旦不使用就会被删除。此外，为防止死锁，获取锁的总顺序是一致的：首先按命名空间树中的层级排序，然后在同一层级内按字典序排序。</p><h3 id=42-副本放置>4.2 副本放置<a hidden class=anchor aria-hidden=true href=#42-副本放置>#</a></h3><p>一个 GFS 集群在多个层面上高度分布化。它通常拥有数百个分布在多个机架上的数据块服务器。这些数据块服务器又可能被来自同一机架或不同机架的数百个客户端访问。位于不同机架的两台机器之间的通信可能需要通过一个或多个网络交换机。此外，进入或离开某个机架的带宽可能小于该机架内所有机器的总带宽。多层级的分布化为数据分布带来了独特的挑战，涉及到可扩展性、可靠性和可用性的需求。</p><p>分块副本放置策略有两个目的：最大化数据的可靠性和可用性，以及最大化网络带宽的利用。为此，仅将副本分布在不同机器上是不够的，这样只能防止磁盘或机器故障并充分利用每台机器的网络带宽。我们还必须将块副本分布在不同的机架上。这可以确保即使整个机架发生故障或下线（例如，由于共享资源故障，如网络交换机或电路问题），块的一些副本仍能存活并保持可用性。这也意味着某个块的流量，尤其是读操作，可以利用多个机架的总带宽。然而，写操作的流量需要通过多个机架，这是我们愿意接受的权衡。</p><blockquote><p>未完&mldr;</p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://fireflyyh.top/tags/%E5%88%86%E5%B8%83%E5%BC%8F/>分布式</a></li><li><a href=https://fireflyyh.top/tags/%E8%AE%BA%E6%96%87/>论文</a></li></ul><nav class=paginav><a class=next href=https://fireflyyh.top/posts/tcpl/ch08/><span class=title>Next »</span><br><span>C语言程序设计第二版课后习题--第八章</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://fireflyyh.top/>Jagger's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function i(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(n),s.appendChild(t)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(s.appendChild(n),e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t)):(s.appendChild(n),e.parentNode.appendChild(t)))})</script></body></html>